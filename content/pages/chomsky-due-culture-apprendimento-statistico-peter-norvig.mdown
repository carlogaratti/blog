Date: 07 Jul 2012
Summary: TODO

# Su Chomsky e le due culture dell'apprendimento statistico #

_Questo post è una traduzione di ["On Chomsky and the Two Cultures of Statistical Learning"][] di Peter Norvig._

Technology Review [sostiene][] che al simposio dal titolo 
["Brain, Minds, and Machines"][] tenuto in occasione dei 150 anni del MIT il
professor Noam Chomsky

> ha deriso i ricercatori in apprendimento automatico che usano metodi
> puramente statistici per produrre comportamenti che imitino qualcosa di
> esistente nel mondo, ma che non provano a comprendere il significato di tali
> comportamenti.

Ora è disponibile la [trascrizione][], perciò citiamo Chomsky stesso:

> È vero che è stato fatto un sacco di lavoro nel provare ad applicare modelli
> statistici a svariati problemi linguistici. Penso che ci sia stato qualche
> successo, ma molti fallimenti. Esiste una nozione di successo ... che penso
> si sia vista per la prima volta nella storia della scienza. Chiamano 
> "successo" l'aver approssimato dati non ancora analizzati.

Questo post discute le affermazioni di Chomsky, specula su quello che avrebbe
potuto intendere e prova a determinarne fondatezza e importanza.

Le osservazioni di Chomsky erano in risposta alla domanda di Steven Pinker sui
successi ottenuti dai modelli probabilistici addestrati con metodi statistici.

1. Cosa intendeva Chomsky? Ha ragione?
2. Cosa è un modello statistico?
3. Quanto successo hanno avuto i modelli statistici del linguaggio?
4. Esiste una nozione comparabile di successo nella storia della scienza?
5. Cosa non piace a Chomsky dei modelli statistici?

## Cosa intendeva Chomsky? Ha ragione? ##

Penso che gli argomenti di Chomsky siano i seguenti:

1. I modelli statistici del linguaggio hanno avuto un successo in senso
ingegneristico, ma questo è irrilevante per la scienza.

2. Modellare accuratamente fatti linguistici è come collezionare farfalle;
ciò che conta nella scienza (e in particolare nella linguistica) sono i
principi sottostanti.

3. I modelli statistici sono incomprensibili; non sono di aiuto
all'intuizione.

4. I modelli statistici possono simulare accuratamente certi fenomeni, ma la
simulazione è fatta in modo totalmente sbagliato; le persone non decidono
quale dovrebbe essere la terza parola di una frase consultando una tabella di
probabilità TODO

5. I modelli statistici si sono dimostrati incapaci di apprendere il
linguaggio; dunque il linguaggio deve essere innato, perciò perché questi
modellatori statistici stanno sprecando il proprio tempo in uno sforzo
sbagliato?

Ha ragione? Questo è un dibattito di lunga data. Di seguito le mie risposte:

1. Sono d'accordo che il successo ingegneristico non sia l'obiettivo o la
misura di buona scienza. Osservo però che la scienza e l'ingegneria si 
sviluppano insieme, e che il successo ingegnerestico indica che qualcosa
sta andando per il verso giusto, perciò è un indizio (ma non una prova) di
un modello scientifico di successo.

2. La scienza è una combinazione di raccolta di dati e formulazione di teorie;
nessuna delle due attività può procedere da sola. Penso che Chomsky abbia 
torto nel far pendere la bilancia a favore della teoria a discapito della
raccolta di dati; nella storia della scienza, TODO

3. Sono d'accordo che sia difficile comprendere un modello contenente miliardi
di parametri. Sicuramente un uomo non può capire un tale modello andando a
indagare individualmente i valori di ogni parametro. Ma si può guadagnare in
intuizione esaminando le _proprietà_ del modello — dove ha successo e dove fallisce, quanto bene impara in funzione dei dati, eccetera.

4. Sono d'accordo che un modello Markoviano di probabilità delle singole
parole non può modellare l'intero linguaggio. È ugualmente vero che un conciso
modello ad albero privo di probabilità non può modellare l'intero linguaggio.
Serve un modello probabilistico che copra parole, alberi, semantica, contesti,
discorsi, eccetera. Chomsky rigetta tutti i modelli probabilistici per via
delle mancanze di particolari modelli di 50 anni fa. Capisco come Chomsky,
partendo dalla studio della generazione del linguaggio, arrivi alla 
conclusione che i modelli probabilistici non siano necessari. Ma la stragrande
maggioranza delle persone che studiano compiti di _interpretazione_, come il
riconoscimento della voce, TODO

5. Nel 1967 il teorema di Gold mostrò alcune limitazioni teoriche sulla
deduzione logica di linguaggi formali. Ma questo risultato non ha niente a che 
vedere con il compito affrontato da chi impara un linguaggio naturale. In ogni
caso, nel 1969 sapevamo già, grazie a Horning, che l'inferenza probabilistica 
di grammatiche libere da contesto non soffre di tale limitazione. Sono 
d'accordo con Chomsky che sia innegabile che gli uomini possiedano una 
capacità naturale di imparare il linguaggio naturale, ma non sappiamo 
abbastanza di questa capacità per escluderne rappresentazioni probabilistiche 
o apprendimento statistico. Penso che sia molto più probabile che 
l'apprendimento del linguaggio umano comporti qualcosa simile all'inferenza 
probabilistica o statistica, ma non lo sappiamo ancora.

Consentitemi ora di giustificare le mie risposte con uno sguardo più attento
alle restanti domande.

## Cosa è un modello statistico? ##

Un **modello statistico** è un modello matematico modificato o addestrato 
dall'aggiunta di dati. I modelli statistici sono spesso, ma non sempre,
probabilistici. Dove è importante distinguere saremo attenti a non dire
soltanto "statistico", ma useremo i seguenti lemmi:

  + Un **modello matematico** specifica una relazione fra variabili, sia in 
  una forma funzionale da input a output (ad esempio, $y = mx + b$) o in forma
  relazionale (ad esempio, le seguenti coppie $(x,y)$ sono parte di una certa
  relazione).
  + Un **modello probabilistico** specifica una distribuzione di probabilità
  $P(x,y)$ su certi possibil valori di alcune variabili aleatorie,
  diversamente da una relazione deterministica del tipo $y = f(x)$.
  + Un **modello addestrato** usa un qualche algoritmo di
  apprendimento/addestramento che prenda in input una collezione di possibili
  modelli e una collezione di dati (ad esempio coppie $(x,y)$) e scelga il
  modello migliore. Spesso questo comporta scegliere parametri (come $m$ e
  $b$ nell'esempio precedente) per mezzo di inferenza statistica.

Per esempio, un decennio prima di Chomsky, Claude Shannon [propose modelli probabilistici della comunicazione][] basati su catene di Markov di parole. Se
avete un vocabolario di $100000$ parole e un modello di Markov del secondo
ordine (cioè in cui la probabilità di una parola dipende dalle precedenti
due), avrete bisogno di specificare $10^ {15}$ probabilità per determinare il
modello. L'unica maniera fattibile per imparare questi $10^ {15}$ parametri
consiste nel raccogliere statistiche dai dati e usare un qualche metodo di
approssimazione per i molti casi in cui non esistono dati. Perciò molti, ma
non tutti, modelli probabilistici sono addestrati. Inoltre molti, ma non
tutti, modelli addestrati sono probabilistici.

Come ulteriore esempio consideriamo il modello Newtoniano dell'attrazione
gravitazionale, il quale afferma che la forza fra due oggetti di masse $m_1$ e
$m_2$ a distanza $r$ sia data da 

$$F = G\frac{m_1 m_2}{r^ 2}$$

dove $G$ è la costante di gravitazione universale. Questo è un modello
addestrato perché tale costante è stata determinata statisticamente per mezzo
di batterie di esperimenti che soffrivano di errore sperimentale stocastico. È
anche un modello deterministico (non probabilistico) perché stabilisce
un'esatta relazione funzionale. Credo che Chomsky non abbia obiezioni verso
questo tipo di modello statistico. Piuttosto, sembra rivolgere le proprie
critiche a modelli statistici come quello di Shannon che hanno miliardi e
miliardi di parametri, non soltanto uno o due.

(Questo esempio ci permette di introdurre un'ulteriore distinzione: il modello
gravitazionale è **continuo** e **quantitativo**, mentre invece la tradizione
linguistica ha favorito modelli che siano **discreti**, **categorici** e
**qualitativi**: una parola è o meno un verbo, non ci chiediamo quanto lo sia.
Per maggiori dettagli su questa distinzione, consultate l'articolo di Chris 
Manning sulla [sintassi probabilistica][].)

Un modello probabilistico e statistico di interesse è la [legge dei gas perfetti], che descrive la pressione $P$ di un gas in termini del numero di
molecole $N$, della temperatura $T$ e della costante di Boltzmann $k$:

$$P = \frac{Nk\,T}{V}\text{.}$$

Questa equazione può essere ricavata da principi primi con i mezzi della
meccanica statistica. È un modello stocastico e non esatto; il _vero_ modello
dovrebbe describere i moti di ogni singola particella di gas. Questo modello
ignora tale complessità e _riassume_ TODO

Ora invece consideriamo il modello non statistico dell'ortografia inglese
espresso dalla regola _"I before E except after C."_ Confrontiamolo con il 
seguente modello probabilistico, addestrato e statistico:

$$
\begin{align}
  P(\text{IE})& =0.0177& P(\text{CIE})& =0.0014& P(\text{\*IE})& =0.163&\\\
  P(\text{EI})& =0.0046& P(\text{CEI})& =0.0005& P(\text{\*EI})& =0.0041&
\end{align}
$$

Questo modello deriva dall'analisi statistica di un [corpus di mille miliardi di parole][] inglesi. La notazione $P(\text{IE})$ indica la probabilità che 
una parola di questo contenga le lettere consecutive $\text{IE}$. Allo stesso
modo $P(\text{CIE})$ denota la probabilità che una parola contenga le lettere
$\text{CIE}$, mentre $P(\text{\*IE})$ indica la probabilità di qualunque
lettera diversa da $\text{C}$ seguita da $\text{IE}$. I dati confermano che
$\text{IE}$ è effettivamente più comune di $\text{EI}$, e che il dominio di
$\text{IE}$ si attenua quando preceduto da $\text{C}$, ma, contrariamente alla
regola, $\text{CIE}$ è comunque più comune di $\text{CEI}$. Esempi di parole
contenenti $\text{CIE}$ sono "science", "society", "ancient", "species". Lo
svantaggio della regola _"I before E except after C"_ è che soffre di scarsa
accuratezza. Per esempio:

$$
\begin{align}
  & \text{Accuracy}("\text{I before E}") = \frac{0.0177}{0.0177+0.0046} = 0.793 \\\
  & \text{Accuracy}("\text{I before E except after C}") = \\\ 
  & = \frac{0.0005+0.0163}{0.0005+0.0163+0.0014+0.0041} = 0.753
\end{align}
$$

Un modello statistico più complesso (diciamo, uno che dia le probabilità di
tutte le sequenze di 4 lettere e/o di tutte le parole conosciute) potrebbe
essere [dieci volte più accurato][] nel correggere ortografia, ma offre meno
**intuizione** su quello che sta facendo. (L'intuizione vorrebbe un modello
che conosca i fonemi, la sillabazione, l'origine del linguaggio. Un tale
modello potrebbe essere addestrato (o no) e probabilistico (o no))

Come ultimo esempio (non di modelli statistici, ma per aiutare l'intuzione),
consideriamo la Teoria delle Strette di Mano fra Giudici della Corte Suprema:
quando si riuniscono, tutti i giudici stringono la mano con ogni altro 
giudice. Il numero di partecipanti, $n$, deve essere un intero compreso fra 
0 e 9; qual è il numero totale di strette di mano $h$ per un dato $n$?
Ecco tre possibili spiegazioni:

1. Ognuno degli $n$ giudici stringe la mano agli altri $n-1$ giudici, ma così
stiamo contando le strette di mano fra Alito/Breyer e Breyer/Alito come due
strette di mano distinte, perciò dobbiamo dividere il totale a metà, e quindi
otteniamo $h = \frac{n(n-1)}{2}$.

2. Per evitare di contare due volte ogni stretta ordiniamo i giudici per età e
contiamo TODO

3. Consultiamo questa tabella: $$\begin{matrix} n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\ \hline h & 0 & 0 & 1 & 3 & 6 & 10 & 15 & 21 & 28 & 36\end{matrix}$$

Alcune persone preferiranno la prima spiegazione, alcune la seconda, e chi è
lento a fare addizioni e moltiplicazioni potrebbe preferire la terza. Perché?
Tutte e tre le spiegazioni descrivono _esattamente la stessa teoria_ — la
stessa funzione da $n$ a $h$, definita sullo stesso dominio di possibili
valori di $n$. Quindi potremmo preferire TODO

## Quanto successo hanno avuto i modelli statistici del linguaggio? ##

Chomsky ha detto che i modelli statistici hanno avuto un successo limitato in
certe aree applicative. Andiamo a vedere come i computer approcciano il
linguaggio, e definiamo "successo" come "fare previsioni accurate sul mondo"
Prima ancora le maggiori aree applicative:

  + **Motori di ricerca** TODO
  + **TODO** TODO
  + **Traduzione automatica** TODO
  + **TODO** TODO

Ora invece volgiamo l'attenzione alle parti che sono d'interesse per il
linguista computazionale e non l'utente tipico:

  + **TODO** TODO
  + **TODO** TODO
  + **TODO** TODO
  + **TODO** TODO

Chiaramente non è accurato dire che i modelli statistici (e probabilistici)
hanno ottenuto un successo _limitato_; piuttosto hanno raggiunto una posizione
_dominante_ (ma non esclusiva).

Un'altra misura del successo è quanto un'idea conquisti una comunità di
ricercatori. Come [scrisse][] Steve Abney nel 1996, "In dieci anni i modelli
statistici sono passati dall'essere virtualmente ignoti nella linguistica
computazionale a essere un dato di fatto ... chiunque non sia in grado di
usarne la terminologia in modo convincente corre il rischio di essere
scambiato per lo sguattero di cucina a un banchetto dell'Associazione dei
linguisti computazionali (ACL)".

TODO

TODO

Questa sezione ha mostrato che una ragione per cui la maggior parte dei
ricercatori in linguistica computazionale usa modelli statistici è
_ingegneristica_: possiedono prestazioni allo stato dell'arte, in molti casi
gli altri tipi di modelli sono peggiori. Per il resto di questo post ci
concentreremo sulle ragioni _scientifiche_: sosterremo che i modelli
probabilistici rappresentino meglio i fatti linguistici, e che le tecniche
statistiche ci consentono di comprendere meglio tali fatti.

## Esiste una nozione comparabile di successo nella storia della scienza? ##

Quando Chomsky ha detto _"Questa è una nozione di successo scientifico che
è totalmente nuova. Non conosco niente di simile nella storia della scienza"_
intendeva che TODO

Una [definizione da dizionario][] della scienza è "Lo studio sistematico della
struttura e del comportamento del mondo fisico e naturale attraverso 
osservazioni e esperimenti", il che enfatizza un modello accurato a scapito
dell'intuizione, ma mi pare che entrambe le nozioni siano sempre coesistite.
Per verificarlo ho consultato la rivista per antonomasia, cioè [Science][]. Ho
guardato il numero corrente e ho scelto a caso un titolo e un abstract:

> [Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility][]
> TODO

Sembra proprio che questo articolo si concentri sul "modellare accuratamente
la realtà" piuttosto che "aiutare l'intuizione". TODO

Poi ho guardato tutti i titoli e abstract del [numero corrente][1] di 
_Science_:

  + Comparative Functional Genomics of the Fission Yeasts
  + Dimensionality Control of Electronic Phase Transitions in Nickel-Oxide Superlattices
  + Competition of Superconducting Phenomena and Kondo Screening at the Nanoscale
  + Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility
  + Probing Asthenospheric Density, Temperature, and Elastic Moduli Below the Western United States
  + Impact of Polar Ozone Depletion on Subtropical Precipitation
  + Fossil Evidence on Origin of the Mammalian Brain
  + Industrial Melanism in British Peppered Moths Has a Singular and Recent Mutational Origin
  + The Selaginella Genome Identifies Genetic Changes Associated with the Evolution of Vascular Plants
  + Chromatin "Prepattern" and Histone Modifiers in a Fate Choice for Liver and Pancreas
  + Spatial Coupling of mTOR and Autophagy Augments Secretory Phenotypes
  + Diet Drives Convergence in Gut Microbiome Functions Across Mammalian Phylogeny and Within Humans
  + The Toll-Like Receptor 2 Pathway Establishes Colonization by a Commensal of the Human Microbiota
  + A Packing Mechanism for Nucleosome Organization Reconstituted Across a Eukaryotic Genome
  + Structures of the Bacterial Ribosome in Classical and Hybrid States of tRNA Binding

ho fatto lo stesso per il [numero corrente][2] di _Cell_:

  + Mapping the NPHP-JBTS-MKS Protein Network Reveals Ciliopathy Disease Genes and Pathways
  + Double-Strand Break Repair-Independent Role for BRCA2 in Blocking Stalled Replication Fork Degradation by MRE11
  + Establishment and Maintenance of Alternative Chromatin States at a Multicopy Gene Locus
  + An Epigenetic Signature for Monoallelic Olfactory Receptor Expression
  + Distinct p53 Transcriptional Programs Dictate Acute DNA-Damage Responses and Tumor Suppression
  + An ADIOL-ERβ-CtBP Transrepression Pathway Negatively Regulates Microglia-Mediated Inflammation
  + A Hormone-Dependent Module Regulating Energy Balance
  + Class IIa Histone Deacetylases Are Hormone-Activated Regulators of FOXO and Mammalian Glucose Homeostasis
  
e infine per i [Premi Nobel 2010][] nelle scienze:

  + Fisica: _Per gli eccezionali esperimenti sulle strutture 2D del Grafene_
  + Chimica: _Per le reazioni di accoppiamento ossidativo nella sintesi
  organica catalizzate mediante il palladio_
  + Fisiologia o Medicina: _Per lo sviluppo della fertilizzazione in vitro_

La mia conclusione è che il 100% di questi premi e articoli riguardano più che
altro "modellare accuratamente il mondo" piuttosto che "aiutare l'intuizione",
sebbene possegganno una componente teorica. Mi rendo conto che discriminare
fra le due alternative sia un compito difficile e mal definito, e che non
dovreste accettare il mio giudizio perché evidentemente di parte. (Ho pensato
di fare un esperimento con Mechanical Turk per ottenere una risposta
imparziale, ma chi è familiare con questo strumento mi ha detto che queste
domande sarebbero state probabilmente troppo difficili. Perciò il lettore può
fare il proprio esperimento e vedere se concorda).

## Cosa non piace a Chomsky dei modelli statistici? ##

Ho detto che i modelli statistici vengono talvolta confusi con i modelli
probabilistici; consideriamo dunque in quale misura le obiezioni di Chomsky
siano piuttosto rivolte verso i modelli probabilistici. È ben noto che nel 
1969 [abbia scritto][]:

> Ma deve essere riconosciuto che la nozione di "probabilità di una frase" sia
> interamente inutile, sotto qualunque interpretazione.

Il suo argomento principale era che, per ogni interpretazione a lui nota, la
probabilità di una frase mai vista deve essere zero, e, poiché frasi mai viste
vengono continuamente generate, ci deve essere una contraddizione. La
soluzione è ovviamente che non è necessario assegnare una probabilità nulla a
una frase mai vista; in effetti è ben noto come assegnare loro una
probabilità non nulla con i metodi probabilistici odierni, quindi questa
critica non è valida, ma è stata molto influente per decenni. Prima di ciò in
[Syntactic Structures][] (1957) Chomsky scrisse:

> Penso che siamo costretti a concludere che ... i metodi probabilistici non
> aiutino in alcun modo l'intuizione a risolvere semplici problemi di
> struttura sintattica.

Nella nota a questa conclusione Chomsky ipotizza la possibilità di un modello
statistico/probabilistico utile, dicendo "Non vorrei certo suggerire che sia 
impensabile, ma non conosco alcuna proposta priva di ovvie mancanze". Ecco la 
principale "ovvia mancanza". [Considerate:][]

1. **I** never, ever, ever, ever, ... **fiddle** around in any way with electrical equipment.
2. **She** never, ever, ever, ever, ... **fiddles** around in any way with electrical equipment.
3. **I** never, ever, ever, ever, ... **fiddles** around in any way with electrical equipment.
4. **She** never, ever, ever, ever, ... **fiddle** around in any way with electrical equipment.

Non importa quante ripetizioni di "ever" uno inserisca, le prime due frasi
sono grammaticalmente corrette, le ultime due no. Un modello probabilistico
Markoviano con $n$ stati non potrà mai fare la distinzione necessaria quando
ci sono più di $n$ copie di "ever". Dunque, un modello probabilistico
Markoviano non descrive l'intera lingua inglese.

Questa argomento è corretto, ma è una critica dei modelli Markoviani — non ha
niente a che vedere con i modelli probabilistici (o addestrati). Per di più
dal 1957 sono noti molti modelli probabilistici oltre a quelli Markoviani. Gli
esempi precedenti possono essere fra loro distinti con un modello a stati
finiti che non sia una catena, ma altri esempi richiedono modelli più
sofisticati. I più studiati sono le grammatiche probabilistiche libere da 
contesto (PCFG), le quali operano su alberi, categorie di parole, singoli
elementi lessicali, e nessuna possiede le restrizioni dei modelli a stati
finiti. Scopriamo che le PCFG sono lo stato dell'arte per la velocità di
parsing e sono più semplici da imparare dai dati delle grammatiche libere da
contesto. Altri tipi di modelli probabilistici coprono la semantica e le
strutture del discorso. Ogni modello probabilistico contiene un modello
deterministico (perché questo può essere pensato come un modello 
probabilistico con le sole probabilità 0 e 1), quindi ogni critica valida dei 
modelli probabilistici dovrebbe riguardare il fatto che siano troppo 
espressivi, non il contrario.

In _Syntactic Structures_ Chomsky introdusse un esempio oggi famoso di critica
ai modelli probabilistici a stati finiti:

> Nè (a) 'colorless green ideas sleep furiosly' nè (b) 'furiosly sleep ideas
> green colorless', o una qualunque delle loro parti, sono mai apparse nell'
> esperienza linguistica di chi parla inglese. Ma (a) è grammaticalmente
> corretta, (b) no.

Sembra che Chomsky avesse ragione a sostenere che tali frasi non fossero mai
state pubblicate prima del 1955. Non sono sicuro che cosa intendesse con "una
qualunque delle loro parti", ma ogni coppia di parole contigue è apparsa, per
esempio:

  + "It is neutral green, **colorless green**, like the glacous water lying
  in a cellar." [The Paris we remember][], Elisabeth Finley Thomas (1942).
  + "To specify those **green ideas** is hardly necessary, but you may observe
  Mr. [D.H.] Lawrence in the role of the satiated aesthete." [The New Republic: Volume 29][] p. 184, William White (1922).
  + "**Ideas sleep** in books." [Current Opinion: Volume 52][], (1912).

Ma a prescindere da cosa si intendesse con "parte", un modello a stati 
finiti statisticamente addestrato _può_ distinguere fra le due frasi. Pereira
(2001) [mostrò][] che un tale modello, arricchito con categorie di parole e
addestrato TODO

TODO

TODO

TODO

1. The earth quaked.
2. ? It quaked her bowels

TODO

TODO

TODO

TODO

D'altro canto Ernest Rutherford (fisico, 1871-1937) disprezza la semplice
descrizione, dicendo "La scienza è fisica oppure collezione di francobolli".
Chomsky è con lui: "Puoi anche collezionare farfalle e fare tante
osservazioni. Se ti piacciono le farfalle va tutto bene, ma questo non deve
essere confuso con la ricerca, il cui scopo è la scoperta di principi
unificanti".

TODO

## Le due culture ##

Dopo aver dato spazio a tutti questi importanti scienziati, penso che il
contributo più rilevante a questa discussione sia stato dato nell'articolo
del 2001 di Leo Breiman (statistico, 1928-2005) dal titolo [Statistical Modeling: The Two Cultures][]. In questo articolo Breiman, con allusione a
C.P. Snow, descrive due culture:

TODO

TODO

Sembra che Chomsky sollevi critiche principalmente contro la cultura della
modellazione algoritmica. Non è solo perché i modelli sono statistici (o
probabilistici), è anche perché il loro prodotto modella accuratamente la
realtà ma non è interpretabile facilmente dagli uomini, e non vuole
corrispondere al processo verificatosi in natura. In altre parole, la
modellazione algoritmica descrive _cosa_ accade, ma non spiega _perché_.

TODO

Nel gennaio 2011 il personaggio televisivo Bill O'Reilly è intervenuto in più
di una guerra culturale con la propria frase ["La marea sale, la mare scende. Mai un malinteso. Nessuno è in grado di spiegarlo"][], a suo dire un argomento 
per l'esistenza di Dio. TODO

TODO

TODO

Il problema è che la realtà è più complicata della sua teoria. Ecco dei
pronomi mancanti in inglese:

  + "Not gonna do it. Wouldn't be prudent." (Dana Carvey, [impersonating George H. W. Bush][])
  + "Thinks he can outsmart us, does he?" (Evelyn Waugh, [The Loved One][])
  + "Like to fight, does he?" (S.M. Stirling, [The Sunrise Lands][])
  + "Thinks he's all that." (Kate Brian, [Lucky T][])
  + "Go for a walk?" (infiniti proprietari di cani)
  + "Gotcha!" "Found it!" "Looks good to me!" (espressioni comuni)

I linguisti possono discutere sull'interpretazione di questi fatti per ore, ma
la diversità dei linguaggi sembrano molto più complesse di un singolo valore
booleano di un parametro pro-drop. Non dovremmo accettare una struttura
teorica che prioritizzi un modello semplice a uno che rifletta accuratamente
la realtà.

TODO

Infine, un'ulteriore ragione per cui a Chomsky non piacciono i modelli
statistici è che tendono a rendere la linguistica una scienza empirica (una
scienza su come le persone usano davvero il linguaggio) piuttosto che una
scienza matematica (uno studio delle proprietà matematiche dei modelli del
linguaggio formale). Chomsky preferisce la seconda, come evidente da questa
frase in [Aspects of the Theory of Syntax][] (1965):

> TODO

TODO

TODO

> La risposta di Platone era che la conoscenza è "ricordata" da un'esistenza
> precedente. La risposta richiede un meccanismo: magari un'anima immortale
> ... 

Era ragionevole che Platone pensasse che l'ideale di, diciamo, un cavallo 
fosse più importante di ciascun cavallo che possa essere percepito. Nel 400 
a.C. le specie erano ritenuto eterne e fisse. Oggi sappiamo che non è vero; i 
cavalli di un altro muro di caverna — quelli di Lascaux — sono oggi estinti, e 
che i cavalli odierni continuano a evolvere nel tempo. Quindi non esiste 
qualcosa come una singola, eterna, forma ideale di "cavallo". 

TODO

## Bibliografia annotata ##

1. Abney, Steve (1996) [Statistical Methods and Linguistics][], in Klavans e Resnick (ed.) _The Balancing Act: Combining Symbolic and Statistical Approaches to Language_, MIT Press.
   > Un'introduzione eccellente e completa all'approccio statistico per la
   > trattazione del linguaggio, che inoltre comprende alcuni argomenti non
   > trattati spesso come l'evoluzione del linguaggio e le differenze
   > individuali.
   
2. Breiman, Leo (2001) [Statistical Modeling: The Two Cultures][], _Statistical Science_, Vol. 16, No. 3, 199-231.
   > Breiman descrive magnificamente i due approcci, spiegando i benefici del
   > proprio e difendendo i propri argomenti con le testimonianze di eminenti
   > statistici: Cox, Efron, Hoadley e Parzen.
   
3. Chomsky, Noam (1956) [Three Models for the Description of Language][], _IRE Transactions on Information theory_ (2), pp. 113-124.
   > Vengono comparate le grammatiche a stati finiti, a struttura di frase e
   > trasformazionali. Viene introdotta la frase "colorless green ideas sleep 
   > furiosly".
   
4. Chomsky, Noam (1967) [Syntactic Structures][], Mouton.
   > Un'esposizione in formato libro della teoria di Chomsky che fu la
   > dottrina principale in linguistica per un decennio. Viene sostenuto che
   > i modelli probabilistici non forniscono intuizione per la sintassi.
   
5. Chomsky, Noam (1969) [Some Empirical Assumptions in Modern Philosophy of Language][], in _Philosophy, Science and Method: Essays in Honor or Ernest Nagel_, St. Martin's Press.
   > Viene sostenuto che la nozione di "probabilità di una frase" sia
   > totalmente inutile.
   
6. Chomsky, Noam (1981) [Lectures on government and binding][], de Gruyer.
   > Una revisione della teoria di Chomsky; viene introdotta la Grammatica
   > Universale. La citiamo per l'inclusione di parametri come "pro-drop".
   
7. Chomsky, Noam (1991) [Linguistics and adjacent fields: a personal view][], in Kasher (ed.), _A Chomskyan Turn_, Oxford.
   > Ho trovato le citazioni su Platone in questo articolo pubblicato dal
   > Partito Comunista della Gran Bretagna e scritto da una persona 
   > apparentemente a digiuno di concetti di linguistica ma con una precisa 
   > agenda politica.
   
8. Gold, E. M. (1967) [Language Identification in the Limit][], _Information and Control_, Vol. 10, No. 5, pp. 447-474.
   > Gold dimostrò un risultato nella teoria dei linguaggi formali che
   > possiamo così enunciare (con qualche licenza): supponete di avere un
   > gioco fra due giocatori, chi indovina e chi sceglie. Il secondo dice al
   > primo "Ecco un numero infinito di linguaggi. Ne sceglierò uno e ti
   > leggerò frasi da questo linguaggio. A un certo tuo compleanno ti darò un
   > test del tipo Vero/Falso composto da 100 frasi che non hai ancora
   > sentito e tu dovrai dire se quelle frasi appartengono al linguaggio
   > oppure no." Ci sono certe condizioni su come sia fatto questo insieme 
   > infinito e come possano essere scelte le frasi (può volontariamente 
   > sviare, ma non può ad esempio ripetere la stessa frase per sempre). Il 
   > risultato di Gold è che se l'insieme infinito è composto da linguaggi 
   > liberi da contesto allora non esiste una strategia per cui chi indovina 
   > sia in grado di dare la risposta giusta a tutte le 100 domande, non 
   > importa quanto in là sia il giorno del test. Chomsky e altri hanno 
   > interpretato questo risultato come affermante che è impossibile che un
   > bambino impari il linguaggio umano senza possedere un innato "organo del 
   > linguaggio". Ma, come [Johnson (2004)][] e altri dimostrano, questa è una 
   > conclusione non valida; il compito di azzeccare tutte le domande del test 
   > (definito da Gold "Identificazione del linguaggio") non ha in realtà 
   > niente a che vedere con il compito di acquisizione del linguaggio svolto 
   > dai bambini, quindi il teorema di Gold non si applica.
     
9. Horning, J. J. (1969) [A study of grammatical inference][], Tesi di Dottorato, Stanford Univ.
   > Se Gold trovò un risultato negativo — cioè che i linguaggi liberi da
   > contesto non sono identificabili tramite esempi — Horning invece trovò
   > un risultato positivo — cioè che i linguaggi probabilistici liberi da
   > contesto invece lo sono (a meno di un errore arbitrariamente piccolo).
   > Nessuno dubita che gli uomini posseggano una capacità unica e innata di
   > comprendere il linguaggio (sebbene sia ignoto in quale misura queste
   > capacità siano specifiche per il linguaggio o siano abilità cognitive
   > generiche di produzione di astrazioni e ordinamento). Horning tuttavia 
   > dimostrò nel 1969 che il teorema di Gold non possa essere usato come 
   > argomento convincente per sostenere l'esistenza di un organo del 
   > linguaggio innato, il quale specifichi l'intero linguaggio tranne qualche 
   > parametro.
   
10. Johnson, Kent (2004) [Gold's Theorem and cognitive science][], _Philosophy of Science_, Vol. 71, pp. 571-592.
    > Il miglior articolo che abbia letto sul vero contenuto del teorema di
    > Gold e su quello che è stato detto in proposito (correttamente e non).
    > Si conclude che il teorema di Gold riguarda il linguaggi formali, ma non
    > l'acquisizione del linguaggio.

11. Lappin, Shalom and Shieber, Stuart M. (2007) [Machine learning theory and practice as a source of insight into universal grammar][], _Journal of Linguistics_, Vol. 43, No. 2, pp. 393-427.
    > TODO. Propone alternative al modello della Grammatica Universale 
    > consistente di un insieme fissato di parametri binari.

12. Manning, Christopher (2002) [Probabilistic Syntax][], in Bod, Hay, e Jannedy (ed.), _Probabilistic Linguistics_, MIT Press.
    > Un'introduzione affascinante alla sintassi probabilistica, e a come sia
    > un modello migliore per fatti linguistici rispetto alla sintassi per
    > categorie. Contiene inoltre "le gioie e i pericoli della linguistica su
    > corpus".

13. Norvig, Peter (2007) [How to Write a Spelling Corrector][], pagina web.
    > Mostra del codice funzionante per implementare un algoritmo di 
    > correzione ortografica probabilistico e statistico.

14. Norvig, Peter (2009) [Natural Language Corpus Data][], in Seagran and Hammerbacher (ed.), _Beautiful Data_, O'Reilly.
    > Una versione espansa del precedente articolo; mostra come implementare
    > tre compiti: divisione del testo, decodifica crittografica e correzione
    > ortografica (in modo leggermente più completo del precedente articolo).

15. Pereira, Fernando (2002) [Formal grammar and information theory: together again?][], in Nevin and Johnson (ed.), _The Legacy of Zellig Harris_, Benjamins.
    > Quando ho cominciato a scrivere il post che state leggendo in questo
    > momento mi sono concentrato su eventi che accadevano a Cambridge,
    > Massachusetts, a 4800 chilometri da casa. Dopo aver fatto un po' di
    > ricerca mi ha sopreso scoprire che gli autori di due dei tre migliori         articoli su questo argomento sedevano a 10 metri dalla mia scrivania:
    > Fernando Pereira e Chris Manning. (Il terzo, Steve Abney, sta a 3700
    > chilometri di distanza.) Ma forse non mi sarei dovuto stupire. Ricordo
    > una volta in cui tenni una conferenza all'Associazione dei Linguisti
    > Computazionali sui modelli del linguaggio usati a Google basati su 
    > corpus, e Fernando, allora professore all'università della Pennsylvania,
    > commentò "Mi sento come se io fossi un fisico delle particelle e tu
    > avessi l'unico acceleratore esistente". Qualche anno dopo arrivò a
    > Google. Fernando è anche famoso per la frase "Più vecchio divento, più
    > giù vado nella Gerarchia di Chomsky". Il suo articolo parla
    > sostanzialmente delle stesse cose del presente post, ma si spinge più in
    > profondità nello spiegare i vari modelli probabilistici disponibili e
    > in che modo siano utili.

16. Platone (c. 380AC) [La Repubblica][]
    > Qui citato per il mito della caverna.

17. Shannon, C.E. (1948) [A Mathematical Theory of Communication][], _The Bell System Technical Journal_, Vol. 27, pp. 379-423, 623-656.
    > Un articolo enormemente influente che dette inizio al campo della Teoria
    > dell'informazione, introdusse il termine "bit" e il modello di canale con rumore, dimostrò l'approssimazione dell'inglese per n-grammi,
    > descrisse modelli Markoviani del linguaggio, definì l'entropia rispetto
    > a questi modelli, e permise la crescita dell'industria delle
    > telecomunicazioni.

["On Chomsky and the Two Cultures of Statistical Learning"]: http://norvig.com/chomsky.html

[sostiene]: http://www.technologyreview.com/computing/37525/?a=f
["Brain, Minds, and Machines"]: http://mit150.mit.edu/symposia/brains-minds-machines
[trascrizione]: http://languagelog.ldc.upenn.edu/myl/PinkerChomskyMIT.html
[propose modelli probabilistici della comunicazione]: http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf
[sintassi probabilistica]: http://nlp.stanford.edu/~manning/papers/probsyntax.pdf
[legge dei gas perfetti]: http://it.wikipedia.org/wiki/Equazione_di_stato_dei_gas_perfetti
[corpus di mille miliardi di parole]: http://norvig.com/ngrams/
[dieci volte più accurato]: http://norvig.com/spell-correct.html

[scrisse]: http://www.vinartus.net/spa/95c.pdf

[definizione da dizionario]: http://www.google.com/webhp?sourceid=chrome-instant&ie=UTF-8&ion=1&nord=1#sclient=psy&hl=en&tbo=1&nord=1&tbs=dfn:1&source=hp&q=science&aq=f&aqi=p-p2g-e2g-c2g2g-c1g1&aql=&oq=&pbx=1&tbo=1&bav=on.2,or.r_gc.r_pw.&fp=18c4c2d0f0f5fea9&biw=1186&bih=634
[Science]: http://www.sciencemag.org/
[Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility]: http://www.sciencemag.org/content/332/6032/944.abstract
[1]: http://www.sciencemag.org/content/332/6032.toc
[2]: http://www.cell.com/issue?pii=S0092-8674(11)X0010-7
[Premi Nobel 2010]: http://nobelprize.org/nobel_prizes/lists/year/

[abbia scritto]: http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false
[Considerate:]: http://books.google.com/books?id=BANsZLg1uy4C&lpg=PP1&dq=unfortunate%20events%20reptile&pg=PA135#v=onepage&q=ever%20ever%20ever&f=false
[Current Opinion: Volume 52]: http://books.google.com/books?id=fTciAQAAIAAJ&pg=PA96&dq=%22ideas+sleep%22&hl=en&ei=bZrcTePRBeHmiAKp2fHoDw&sa=X&oi=book_result&ct=result&resnum=3&sqi=2&ved=0CDoQ6AEwAg#v=onepage&q=%22ideas%20sleep%22&f=false

[Statistical Methods and Linguistics]: http://www.vinartus.net/spa/95c.pdf
[Statistical Modeling: The Two Cultures]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009213726
[Three Models for the Description of Language]: http://www.chomsky.info/articles/195609--.pdf
[Syntactic Structures]: http://books.google.com/books?id=SNeHkMXHcd8C&printsec=frontcover&dq=syntactic+structures+chomsky&hl=en&src=bmrr&ei=1WvcTa3UDeLQiAK6-4zpDw&sa=X&oi=book_result&ct=book-thumbnail&resnum=1&ved=0CD4Q6wEwAA#v=onepage&q=probabilistic&f=false
[Some Empirical Assumptions in Modern Philosophy of Language]: http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false
[Lectures on government and binding]: http://books.google.com/books?id=l08tpkOOdNQC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q=pro-drop&f=false
[Linguistics and adjacent fields: a personal view]: http://www.cpgb.org.uk/article.php?article_id=1004261#23
[Language Identification in the Limit]: http://en.wikipedia.org/wiki/Language_identification_in_the_limit
[Johnson (2004)]: http://psyling.psy.cmu.edu/papers/years/2004/logical/gold-johnson.pdf
[A study of grammatical inference]: http://portal.acm.org/citation.cfm?id=905718&coll=DL&dl=GUIDE&CFID=23829333&CFTOKEN=19917759
[Gold's Theorem and cognitive science]: http://psyling.psy.cmu.edu/papers/years/2004/logical/gold-johnson.pdf
[Machine learning theory and practice as a source of insight into universal grammar]: http://www.dcs.kcl.ac.uk/staff/lappin/papers/lappin-shieber_learning07.pdf
[Probabilistic Syntax]: http://nlp.stanford.edu/~manning/papers/probsyntax.pdf
[How to Write a Spelling Corrector]: http://norvig.com/spell-correct.html
[Natural Language Corpus Data]: http://norvig.com/ngrams/
[Formal grammar and information theory: together again?]: http://www.cis.upenn.edu/~pereira/papers/rsoc.pdf
[La Repubblica]: http://en.wikipedia.org/wiki/Allegory_of_the_Cave
[A Mathematical Theory of Communication]: http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf