Date: 09  Jul 2012
Summary: Technology Review sostiene che il professor Noam Chomsky abbia deriso i ricercatori in apprendimento automatico che usano metodi puramente statistici per produrre comportamenti che imitino qualcosa di esistente nel mondo, ma che non provano a comprendere il significato di tali comportamenti. Questo post discute le affermazioni di Chomsky, specula su quello che avrebbe potuto intendere e prova a determinarne fondatezza e importanza.

# Su Chomsky e le due culture dell'apprendimento statistico #

_Questo post è una traduzione di ["On Chomsky and the Two Cultures of Statistical Learning"][] di Peter Norvig._

Technology Review [sostiene][] che al simposio dal titolo 
["Brain, Minds, and Machines"][] tenuto in occasione dei 150 anni del MIT il
professor Noam Chomsky

> ha deriso i ricercatori in apprendimento automatico che usano metodi
> puramente statistici per produrre comportamenti che imitino qualcosa di
> esistente nel mondo, ma che non provano a comprendere il significato di tali
> comportamenti.

Ora è disponibile la [trascrizione][], perciò citiamo Chomsky stesso:

> È vero che è stato fatto un sacco di lavoro nel provare ad applicare modelli
> statistici a svariati problemi linguistici. Penso che ci sia stato qualche
> successo, ma molti fallimenti. Esiste una nozione di successo ... che penso
> si sia vista per la prima volta nella storia della scienza. Chiamano 
> "successo" l'aver approssimato dati non ancora analizzati.

Questo post discute le affermazioni di Chomsky, specula su quello che avrebbe
potuto intendere e prova a determinarne fondatezza e importanza.

Le osservazioni di Chomsky erano in risposta alla domanda di Steven Pinker sui
successi ottenuti dai modelli probabilistici addestrati con metodi statistici.

1. Cosa intendeva Chomsky? Ha ragione?
2. Cosa è un modello statistico?
3. Quanto successo hanno avuto i modelli statistici del linguaggio?
4. Esiste una nozione comparabile di successo nella storia della scienza?
5. Cosa non piace a Chomsky dei modelli statistici?

## Cosa intendeva Chomsky? Ha ragione? ##

Penso che gli argomenti di Chomsky siano i seguenti:

1. I modelli statistici del linguaggio hanno avuto un successo in senso
ingegneristico, ma questo è irrilevante per la scienza.

2. Modellare accuratamente fatti linguistici è come collezionare farfalle;
ciò che conta nella scienza (e in particolare nella linguistica) sono i
principi sottostanti.

3. I modelli statistici sono incomprensibili; non sono di aiuto
all'intuizione.

4. I modelli statistici possono simulare accuratamente certi fenomeni, ma la
simulazione è fatta in modo totalmente sbagliato; le persone non decidono
quale dovrebbe essere la terza parola di una frase consultando una tabella di
probabilità alla riga corrispondente alle due parole precedenti e poi
pronunciando la parola seguente. Questo compito viene svolto senza avvalersi
di probabilità o statistica.

5. I modelli statistici si sono dimostrati incapaci di apprendere il
linguaggio; dunque il linguaggio deve essere innato, perciò perché questi
modellatori statistici stanno sprecando il proprio tempo in uno sforzo
sbagliato?

Ha ragione? Questo è un dibattito di lunga data. Di seguito le mie risposte:

1. Sono d'accordo che il successo ingegneristico non sia l'obiettivo o la
misura di buona scienza. Osservo però che la scienza e l'ingegneria si 
sviluppano insieme, e che il successo ingegnerestico indica che qualcosa
sta andando per il verso giusto, perciò è un indizio (ma non una prova) di
un modello scientifico di successo.

2. La scienza è una combinazione di raccolta di dati e formulazione di teorie;
nessuna delle due attività può procedere da sola. Penso che Chomsky abbia 
torto nel far pendere la bilancia a favore della teoria a discapito della
raccolta di dati; nella storia della scienza, la faticosa raccolta dei dati è 
l'attività prevalente, non una novità. La scienza della comprensione del 
linguaggio non è diversa dalle altre scienze in questo.

3. Sono d'accordo che sia difficile comprendere un modello contenente miliardi
di parametri. Sicuramente un uomo non può capire un tale modello andando a
indagare individualmente i valori di ogni parametro. Ma si può guadagnare in
intuizione esaminando le _proprietà_ del modello — dove ha successo e dove fallisce, quanto bene impara in funzione dei dati, eccetera.

4. Sono d'accordo che un modello Markoviano di probabilità delle singole
parole non può modellare l'intero linguaggio. È ugualmente vero che un conciso
modello ad albero privo di probabilità non può modellare l'intero linguaggio.
Serve un modello probabilistico che copra parole, alberi, semantica, contesti,
discorsi, eccetera. Chomsky rigetta tutti i modelli probabilistici per via
delle mancanze di particolari modelli di 50 anni fa. Capisco come Chomsky,
partendo dalla studio della generazione del linguaggio, arrivi alla 
conclusione che i modelli probabilistici non siano necessari. Ma la stragrande
maggioranza delle persone che studiano compiti di _interpretazione_, come il
riconoscimento della voce, concludono rapidamente che l'interpretazione è un
problema inerentemente probabilistico: dato un flusso di input disturbato da
rumore, cosa è più probabile che sia stato detto? Einsten disse di rendere
tutto il più semplice possibile, ma non più semplice. Molti fenomeni nelle
scienze sono stocastici, e il modello più semplice per essi è probabilistico;
io credo che il linguaggio sia uno di essi, perciò i modelli probabilistici
sono il modo migliore per rappresentare il linguaggio, per operare su di esso
algoritmicamente, per capire come gli uomini lo comprendano a propria volta.

5. Nel 1967 il teorema di Gold mostrò alcune limitazioni teoriche sulla
deduzione logica di linguaggi formali. Ma questo risultato non ha niente a che 
vedere con il compito affrontato da chi impara un linguaggio naturale. In ogni
caso, nel 1969 sapevamo già, grazie a Horning, che l'inferenza probabilistica 
di grammatiche libere da contesto non soffre di tale limitazione. Sono 
d'accordo con Chomsky che sia innegabile che gli uomini possiedano una 
capacità naturale di imparare il linguaggio naturale, ma non sappiamo 
abbastanza di questa capacità per escluderne rappresentazioni probabilistiche 
o apprendimento statistico. Penso che sia molto più probabile che 
l'apprendimento del linguaggio umano comporti qualcosa simile all'inferenza 
probabilistica o statistica, ma non lo sappiamo ancora.

Consentitemi ora di giustificare le mie risposte con uno sguardo più attento
alle restanti domande.

## Cosa è un modello statistico? ##

Un **modello statistico** è un modello matematico modificato o addestrato 
dall'aggiunta di dati. I modelli statistici sono spesso, ma non sempre,
probabilistici. Dove è importante distinguere saremo attenti a non dire
soltanto "statistico", ma useremo i seguenti lemmi:

  + Un **modello matematico** specifica una relazione fra variabili, sia in 
  una forma funzionale da input a output (ad esempio, $y = mx + b$) o in forma
  relazionale (ad esempio, le seguenti coppie $(x,y)$ sono parte di una certa
  relazione).
  
  + Un **modello probabilistico** specifica una distribuzione di probabilità
  $P(x,y)$ su certi possibil valori di alcune variabili aleatorie,
  diversamente da una relazione deterministica del tipo $y = f(x)$.
  
  + Un **modello addestrato** usa un qualche algoritmo di
  apprendimento/addestramento che prenda in input una collezione di possibili
  modelli e una collezione di dati (ad esempio coppie $(x,y)$) e scelga il
  modello migliore. Spesso questo comporta scegliere parametri (come $m$ e
  $b$ nell'esempio precedente) per mezzo di inferenza statistica.

Per esempio, un decennio prima di Chomsky, Claude Shannon [propose modelli probabilistici della comunicazione][] basati su catene di Markov di parole. Se
avete un vocabolario di $100000$ parole e un modello di Markov del secondo
ordine (cioè in cui la probabilità di una parola dipende dalle precedenti
due), avrete bisogno di specificare $10^ {15}$ probabilità per determinare il
modello. L'unica maniera fattibile per imparare questi $10^ {15}$ parametri
consiste nel raccogliere statistiche dai dati e usare un qualche metodo di
approssimazione per i molti casi in cui non esistono dati. Perciò molti, ma
non tutti, modelli probabilistici sono addestrati. Inoltre molti, ma non
tutti, modelli addestrati sono probabilistici.

Come ulteriore esempio consideriamo il modello Newtoniano dell'attrazione
gravitazionale, il quale afferma che la forza fra due oggetti di masse $m_1$ e
$m_2$ a distanza $r$ sia data da 

$$F = G\frac{m_1 m_2}{r^ 2}$$

dove $G$ è la costante di gravitazione universale. Questo è un modello
addestrato perché tale costante è stata determinata statisticamente per mezzo
di batterie di esperimenti che soffrivano di errore sperimentale stocastico. È
anche un modello deterministico (non probabilistico) perché stabilisce
un'esatta relazione funzionale. Credo che Chomsky non abbia obiezioni verso
questo tipo di modello statistico. Piuttosto, sembra rivolgere le proprie
critiche a modelli statistici come quello di Shannon che hanno miliardi e
miliardi di parametri, non soltanto uno o due.

(Questo esempio ci permette di introdurre un'ulteriore distinzione: il modello
gravitazionale è **continuo** e **quantitativo**, mentre invece la tradizione
linguistica ha favorito modelli che siano **discreti**, **categorici** e
**qualitativi**: una parola è o meno un verbo, non ci chiediamo quanto lo sia.
Per maggiori dettagli su questa distinzione, consultate l'articolo di Chris 
Manning sulla [sintassi probabilistica][].)

Un modello probabilistico e statistico di interesse è la [legge dei gas perfetti], che descrive la pressione $P$ di un gas in termini del numero di
molecole $N$, della temperatura $T$ e della costante di Boltzmann $k$:

$$P = \frac{Nk\,T}{V}\text{.}$$

Questa equazione può essere ricavata da principi primi con i mezzi della
meccanica statistica. È un modello stocastico e non esatto; il _vero_ modello
dovrebbe describere i moti di ogni singola particella di gas. Questo modello
ignora tale complessità e _riassume_ la nostra incertezza sulla posizione di
ogni singola molecola. Perciò, sebbene sia statistico e probabilistico, 
sebbene non modelli completamente la realtà, è un buon modello sia dal punto
di vista delle previsioni sia dell'intuizione — intuizione che non otterremmo
se provassimo a capire i _veri_ movimenti di ogni singola molecola. 

Ora invece consideriamo il modello non statistico dell'ortografia inglese
espresso dalla regola _"I before E except after C."_ Confrontiamolo con il 
seguente modello probabilistico, addestrato e statistico:

$$
\begin{align}
  P(\text{IE})& =0.0177& P(\text{CIE})& =0.0014& P(\text{\*IE})& =0.163&\\\
  P(\text{EI})& =0.0046& P(\text{CEI})& =0.0005& P(\text{\*EI})& =0.0041&
\end{align}
$$

Questo modello deriva dall'analisi statistica di un [corpus di mille miliardi di parole][] inglesi. La notazione $P(\text{IE})$ indica la probabilità che 
una parola di questo contenga le lettere consecutive $\text{IE}$. Allo stesso
modo $P(\text{CIE})$ denota la probabilità che una parola contenga le lettere
$\text{CIE}$, mentre $P(\text{\*IE})$ indica la probabilità di qualunque
lettera diversa da $\text{C}$ seguita da $\text{IE}$. I dati confermano che
$\text{IE}$ è effettivamente più comune di $\text{EI}$, e che il dominio di
$\text{IE}$ si attenua quando preceduto da $\text{C}$, ma, contrariamente alla
regola, $\text{CIE}$ è comunque più comune di $\text{CEI}$. Esempi di parole
contenenti $\text{CIE}$ sono "science", "society", "ancient", "species". Lo
svantaggio della regola _"I before E except after C"_ è che soffre di scarsa
accuratezza. Per esempio:

$$
\begin{align}
  & \text{Accuracy}("\text{I before E}") = \frac{0.0177}{0.0177+0.0046} = 0.793 \\\
  & \text{Accuracy}("\text{I before E except after C}") = \\\ 
  & = \frac{0.0005+0.0163}{0.0005+0.0163+0.0014+0.0041} = 0.753
\end{align}
$$

Un modello statistico più complesso (diciamo, uno che dia le probabilità di
tutte le sequenze di 4 lettere e/o di tutte le parole conosciute) potrebbe
essere [dieci volte più accurato][] nel correggere ortografia, ma offre meno
**intuizione** su quello che sta facendo. (L'intuizione vorrebbe un modello
che conosca i fonemi, la sillabazione, l'origine del linguaggio. Un tale
modello potrebbe essere addestrato (o no) e probabilistico (o no))

Come ultimo esempio (non di modelli statistici, ma per aiutare l'intuzione),
consideriamo la Teoria delle Strette di Mano fra Giudici della Corte Suprema:
quando si riuniscono, tutti i giudici stringono la mano con ogni altro 
giudice. Il numero di partecipanti, $n$, deve essere un intero compreso fra 
0 e 9; qual è il numero totale di strette di mano $h$ per un dato $n$?
Ecco tre possibili spiegazioni:

1. Ognuno degli $n$ giudici stringe la mano agli altri $n-1$ giudici, ma così
stiamo contando le strette di mano fra Alito/Breyer e Breyer/Alito come due
strette di mano distinte, perciò dobbiamo dividere il totale a metà, e quindi
otteniamo $h = \frac{n(n-1)}{2}$.

2. Per evitare di contare due volte ogni stretta ordiniamo i giudici per età e
contiamo soltanto le strette di mano del tipo membro più anziano/membro più
giovane. Quindi contiamo per ogni giudice le strette di mano con i giudici
più giovani, e otteniamo $h=\sum_ {i=1}^ n {(i-1)}$.

3. Consultiamo questa tabella: $$\begin{matrix} n & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\\ \hline h & 0 & 0 & 1 & 3 & 6 & 10 & 15 & 21 & 28 & 36\end{matrix}$$

Alcune persone preferiranno la prima spiegazione, alcune la seconda, e chi è
lento a fare addizioni e moltiplicazioni potrebbe preferire la terza. Perché?
Tutte e tre le spiegazioni descrivono _esattamente la stessa teoria_ — la
stessa funzione da $n$ a $h$, definita sullo stesso dominio di possibili
valori di $n$. Quindi potremmo preferire la 1 (o la 2) rispetto alla 3 solo
per ragioni esterne alla teoria stessa. Potremmo trovare che le prime due ci
aiutano a comprendere meglio il problema. Sicuramente queste ultime sono più
utili della 3 nel capire cosa succederebbe se il Congresso decidesse di
aggiungere un nuovo giudice alla Corte Suprema. La teoria 1 sarebbe la più
utile se volessimo sviluppare una teoria delle strette di mano fra giocatori
alla fine di una partita di Hockey (quando ogni giocatore stringe la mano con
i membri della squadra opposta), oppure per dimostrare che è pari il numero di
persone che hanno stretto un numero dispari di mani al MIT Symposium.

## Quanto successo hanno avuto i modelli statistici del linguaggio? ##

Chomsky ha detto che i modelli statistici hanno avuto un successo limitato in
certe aree applicative. Andiamo a vedere come i computer approcciano il
linguaggio, e definiamo "successo" come "fare previsioni accurate sul mondo"
Prima ancora le maggiori aree applicative:

  + **Motori di ricerca** Tutti i più importanti sono addestrati e
  probabilistici. Il loro operato non può essere descritto da alcuna funzione
  semplice.
  + **Riconoscimento della voce** Tutti i maggiori sono addestrati e
  probabilistici, per di più basati su modelli di Markov nascosti.
  + **Traduzione automatica** Tutti i partecipanti più agguerriti a
  competizioni come il [NIST][] usano metodi statistici. Alcuni sistemi
  commerciali usano un ibrido fra metodi addestrati e tecniche basate su
  regole di riscrittura. Delle 4000 coppie di linguaggi gestite da sistemi
  di traduzione automatica, i metodi statistici risultano essere i migliori
  per tutte tranne Giapponese/Inglese, per la quale i metodi statistici e
  ibridi hanno più o meno le stesse prestazioni.
  + **Question answering** Questo tipo di applicazione è meno sviluppato, e
  molti sistemi prendono le mosse dall'approccio statistico e probabilistico
  usato dai motori di ricerca. Il sistema [IBM Watson][], che ha recentemente
  vinto a Jeopardy è interamente probabilistico e addestrato, invece [START][]
  di Boris Katz è un ibrido. Tutti comunque usano delle tecniche statistiche.

Ora invece volgiamo l'attenzione alle parti che sono d'interesse per il
linguista computazionale e non l'utente tipico:

  + **Disambiguazione del significato** Tutti i migliori partecipanti a
  [SemEval-2][] hanno usato tecniche statistiche; la maggior parte sono
  probabilistici; alcuni usano un approccio ibrido basato su regole
  estratte da Wordnet.
  + **Risoluzione di riferimenti comuni** La maggior parte dei sistemi
  correntemente utilizzati sono statistici, sebbene dobbiamo menzionare quello
  di [Haghighi e Klein][], che può essere descritto come un sistema ibrido
  principalmente basato su regole di riscrittura piuttosto che addestrato, il
  quale è tanto efficace quanto i migliori sistemi statistici.
  + **Analisi grammaticale** La maggior parte dei sistemi sono statistici.
  Fa eccezione il [Brill Tagger][], un algoritmo ibrido molto efficace:
  impara un insieme di regole deterministiche a partire da dati statistici.
  + **Parsing** Ci sono più sistemi che fanno parsing, e usano tecniche
  differenti. Quasi tutti i [più efficaci][] sono statistici, la maggior parte
  sono [probabilistici][] (con una minoranza consistente di parser 
  deterministici)

Chiaramente non è accurato dire che i modelli statistici (e probabilistici)
hanno ottenuto un successo _limitato_; piuttosto hanno raggiunto una posizione
_dominante_ (ma non esclusiva).

Un'altra misura del successo è quanto un'idea conquisti una comunità di
ricercatori. Come [scrisse][] Steve Abney nel 1996, "In dieci anni i modelli
statistici sono passati dall'essere virtualmente ignoti nella linguistica
computazionale a essere un dato di fatto ... chiunque non sia in grado di
usarne la terminologia in modo convincente corre il rischio di essere
scambiato per lo sguattero a un banchetto dell'Associazione dei linguisti 
computazionali (ACL)".

Ora, ovviamente la maggioranza non regna — solo perché tutti stanno saltando
su un certo carrozzone, questo non lo rende quello del vincitore. Ma io stesso
ho cambiato idea: dopo circa 14 anni in cui ho tentato di far funzionare
modelli del linguaggio basati su regole logiche, ho cominciato ad adottare
approcci probabilistici (grazie a pionieri come Gene Charniak (e Judea Pearl
per la teoria della probabilità) e ai miei colleghi che cominciarono prima di
molti, come Dekai wu). In quel momento ho visto tutti intorno a me fare la
stessa cosa (e non ho visto nessuno fare il passaggio opposto). Abbiamo visto
tutti le limitazioni delle tecniche vecchie e i benefici di quelle nuove.

E, sebbene usare una misura finanziaria del successo possa sembrare cafone e 
anti intellettuale, vale la pena menzionare che la [creatura][] intellettuale 

Questa sezione ha mostrato che una ragione per cui la maggior parte dei
ricercatori in linguistica computazionale usa modelli statistici è
_ingegneristica_: possiedono prestazioni allo stato dell'arte, in molti casi
gli altri tipi di modelli sono peggiori. Per il resto di questo post ci
concentreremo sulle ragioni _scientifiche_: sosterremo che i modelli
probabilistici rappresentino meglio i fatti linguistici, e che le tecniche
statistiche ci consentono di comprendere meglio tali fatti.

## Esiste una nozione comparabile di successo nella storia della scienza? ##

Quando Chomsky ha detto _"Questa è una nozione di successo scientifico che
è totalmente nuova. Non conosco niente di simile nella storia della scienza"_
intendeva che TODO

Una [definizione da dizionario][] della scienza è "Lo studio sistematico della
struttura e del comportamento del mondo fisico e naturale attraverso 
osservazioni e esperimenti", il che enfatizza un modello accurato a scapito
dell'intuizione, ma mi pare che entrambe le nozioni siano sempre coesistite.
Per verificarlo ho consultato la rivista per antonomasia, cioè [Science][]. Ho
guardato il numero corrente e ho scelto a caso un titolo e un abstract:

> [Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility][]
> TODO

Sembra proprio che questo articolo si concentri sul "modellare accuratamente
la realtà" piuttosto che "aiutare l'intuizione". TODO

Poi ho guardato tutti i titoli e abstract del [numero corrente][1] di 
_Science_:

  + Comparative Functional Genomics of the Fission Yeasts
  + Dimensionality Control of Electronic Phase Transitions in Nickel-Oxide Superlattices
  + Competition of Superconducting Phenomena and Kondo Screening at the Nanoscale
  + Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility
  + Probing Asthenospheric Density, Temperature, and Elastic Moduli Below the Western United States
  + Impact of Polar Ozone Depletion on Subtropical Precipitation
  + Fossil Evidence on Origin of the Mammalian Brain
  + Industrial Melanism in British Peppered Moths Has a Singular and Recent Mutational Origin
  + The Selaginella Genome Identifies Genetic Changes Associated with the Evolution of Vascular Plants
  + Chromatin "Prepattern" and Histone Modifiers in a Fate Choice for Liver and Pancreas
  + Spatial Coupling of mTOR and Autophagy Augments Secretory Phenotypes
  + Diet Drives Convergence in Gut Microbiome Functions Across Mammalian Phylogeny and Within Humans
  + The Toll-Like Receptor 2 Pathway Establishes Colonization by a Commensal of the Human Microbiota
  + A Packing Mechanism for Nucleosome Organization Reconstituted Across a Eukaryotic Genome
  + Structures of the Bacterial Ribosome in Classical and Hybrid States of tRNA Binding

ho fatto lo stesso per il [numero corrente][2] di _Cell_:

  + Mapping the NPHP-JBTS-MKS Protein Network Reveals Ciliopathy Disease Genes and Pathways
  + Double-Strand Break Repair-Independent Role for BRCA2 in Blocking Stalled Replication Fork Degradation by MRE11
  + Establishment and Maintenance of Alternative Chromatin States at a Multicopy Gene Locus
  + An Epigenetic Signature for Monoallelic Olfactory Receptor Expression
  + Distinct p53 Transcriptional Programs Dictate Acute DNA-Damage Responses and Tumor Suppression
  + An ADIOL-ERβ-CtBP Transrepression Pathway Negatively Regulates Microglia-Mediated Inflammation
  + A Hormone-Dependent Module Regulating Energy Balance
  + Class IIa Histone Deacetylases Are Hormone-Activated Regulators of FOXO and Mammalian Glucose Homeostasis
  
e infine per i [Premi Nobel 2010][] nelle scienze:

  + Fisica: _Per gli eccezionali esperimenti sulle strutture 2D del Grafene_
  + Chimica: _Per le reazioni di accoppiamento ossidativo nella sintesi
  organica catalizzate mediante il palladio_
  + Fisiologia o Medicina: _Per lo sviluppo della fertilizzazione in vitro_

La mia conclusione è che il 100% di questi premi e articoli riguardano più che
altro "modellare accuratamente il mondo" piuttosto che "aiutare l'intuizione",
sebbene posseggano una componente teorica. Mi rendo conto che discriminare
fra le due alternative sia un compito difficile e mal definito, e che non
dovreste accettare il mio giudizio perché evidentemente di parte. (Ho pensato
di fare un esperimento con Mechanical Turk per ottenere una risposta
imparziale, ma chi è familiare con questo strumento mi ha detto che queste
domande sarebbero state probabilmente troppo difficili. Perciò il lettore può
fare il proprio esperimento e vedere se concorda).

## Cosa non piace a Chomsky dei modelli statistici? ##

Ho detto che i modelli statistici vengono talvolta confusi con i modelli
probabilistici; consideriamo dunque in quale misura le obiezioni di Chomsky
siano piuttosto rivolte verso i modelli probabilistici. È ben noto che nel 
1969 [abbia scritto][]:

> Ma deve essere riconosciuto che la nozione di "probabilità di una frase" sia
> interamente inutile, sotto qualunque interpretazione.

Il suo argomento principale era che, per ogni interpretazione a lui nota, la
probabilità di una frase mai vista deve essere zero, e, poiché frasi mai viste
vengono continuamente generate, ci deve essere una contraddizione. La
soluzione è ovviamente che non è necessario assegnare una probabilità nulla a
una frase mai vista; in effetti è ben noto come assegnare loro una
probabilità non nulla con i metodi probabilistici odierni, quindi questa
critica non è valida, ma è stata molto influente per decenni. Prima di ciò in
[Syntactic Structures][] (1957) Chomsky scrisse:

> Penso che siamo costretti a concludere che ... i metodi probabilistici non
> aiutino in alcun modo l'intuizione a risolvere semplici problemi di
> struttura sintattica.

Nella nota a questa conclusione Chomsky ipotizza la possibilità di un modello
statistico/probabilistico utile, dicendo "Non vorrei certo suggerire che sia 
impensabile, ma non conosco alcuna proposta priva di ovvie mancanze". Ecco la 
principale "ovvia mancanza". [Considerate:][]

1. **I** never, ever, ever, ever, ... **fiddle** around in any way with electrical equipment.
2. **She** never, ever, ever, ever, ... **fiddles** around in any way with electrical equipment.
3. **I** never, ever, ever, ever, ... **fiddles** around in any way with electrical equipment.
4. **She** never, ever, ever, ever, ... **fiddle** around in any way with electrical equipment.

Non importa quante ripetizioni di "ever" uno inserisca, le prime due frasi
sono grammaticalmente corrette, le ultime due no. Un modello probabilistico
Markoviano con $n$ stati non potrà mai fare la distinzione necessaria quando
ci sono più di $n$ copie di "ever". Dunque, un modello probabilistico
Markoviano non descrive l'intera lingua inglese.

Questa argomento è corretto, ma è una critica dei modelli Markoviani — non ha
niente a che vedere con i modelli probabilistici (o addestrati). Per di più
dal 1957 sono noti molti modelli probabilistici oltre a quelli Markoviani. Gli
esempi precedenti possono essere fra loro distinti con un modello a stati
finiti che non sia una catena, ma altri esempi richiedono modelli più
sofisticati. I più studiati sono le grammatiche probabilistiche libere da 
contesto (PCFG), le quali operano su alberi, categorie di parole, singoli
elementi lessicali, e nessuna possiede le restrizioni dei modelli a stati
finiti. Scopriamo che le PCFG sono lo stato dell'arte per la velocità di
parsing e sono più semplici da imparare dai dati delle grammatiche libere da
contesto. Altri tipi di modelli probabilistici coprono la semantica e le
strutture del discorso. Ogni modello probabilistico contiene un modello
deterministico (perché questo può essere pensato come un modello 
probabilistico con le sole probabilità 0 e 1), quindi ogni critica valida dei 
modelli probabilistici dovrebbe riguardare il fatto che siano troppo 
espressivi, non il contrario.

In _Syntactic Structures_ Chomsky introdusse un esempio oggi famoso di critica
ai modelli probabilistici a stati finiti:

> Nè (a) 'colorless green ideas sleep furiosly' nè (b) 'furiosly sleep ideas
> green colorless', o una qualunque delle loro parti, sono mai apparse nell'
> esperienza linguistica di chi parla inglese. Ma (a) è grammaticalmente
> corretta, (b) no.

Sembra che Chomsky avesse ragione a sostenere che tali frasi non fossero mai
state pubblicate prima del 1955. Non sono sicuro che cosa intendesse con "una
qualunque delle loro parti", ma ogni coppia di parole contigue è apparsa, per
esempio:

  + "It is neutral green, **colorless green**, like the glacous water lying
  in a cellar." [The Paris we remember][], Elisabeth Finley Thomas (1942).
  + "To specify those **green ideas** is hardly necessary, but you may observe
  Mr. [D.H.] Lawrence in the role of the satiated aesthete." [The New Republic: Volume 29][] p. 184, William White (1922).
  + "**Ideas sleep** in books." [Current Opinion: Volume 52][], (1912).

Ma a prescindere da cosa si intendesse con "parte", un modello a stati 
finiti statisticamente addestrato _può_ distinguere fra le due frasi. Pereira
(2001) [mostrò][] che un tale modello, arricchito con categorie di parole e
addestrato con l'algoritmo di massimizzazione della speranza su articoli di
giornale, calcola che la frase (a) è 200.000 volte più probabile della (b).
Per dimostrare che questo non derivasse dall'essere apparsa in qualche
giornale, ho ripetuto l'esperimento con un modello più grezzo basato sul
Laplacian Smoothing e nessuna categoria, addestrato sul 
[corpus di Google books][] dal 1800 al 1954, e ho ottenuto che (a) è circa
10.000 volte più probabile. Se avessimo un modello probabilistico su alberi
invece che su sequenze di parole, potremmo svolgere anche meglio il compito di
calcolare quanto una frase sia grammaticalmente corretta.

Inoltre i modelli statistici sono in grado di affermare che entrambe le frasi
sono _estremamente_ improbabili rispetto a, per esempio, "Effective green
products sell well". La teoria di Chomsky, essendo fondata su categorie, non
può fare queste distinzioni; tutto ciò che può distinguere è grammaticalmente
corretta o no.

Un'altra parte dell'obiezione di Chomsky è che "non possiamo seriamente
proporre che un bambino impari i valori di $10 ^9$ parametri in un'infanzia 
che dura $10^ 8$ secondi". (Si osservi come i modelli moderni possiedono molti
più parametri dei $10^ 9$ postulati negli anni '60). Ma ovviamente nessuno
propone che questi parametri vengano imparati uno per uno; la maniera giusta
per imparare è impostare larghe manciate di parametri vicini a zero con una
qualche procedura di smoothing o di regolarizzazione, e aggiornare
continuamente i parametri ad alta probabilità con le nuove osservazioni.
Inoltre nessuno suggerisce che i soli modelli markoviani siano un serio
modello del linguaggio. Ma io (e altri) suggeriamo che i modelli addestrati e
probabilistici siano un modello migliore dei modelli basati su categorie e
non addestrati. E sì, sembra chiaro che un adulto parlante inglese sappia
miliardi di fatti sulla lingua (per esempio che si dica "big game" invece di
"large game" per descrivere un'importante partita di football). Questi fatti
devono essere in qualche modo codificati nel cervello.

Sembra chiaro che i modelli probabilistici siano migliori per giudicare la
plausibilità di una frase o l'emotività in essa contenuta. E anche se non
foste interessati a questi fattori ma soltanto all'essere grammaticalmente
corretta, sembra comunque che i modelli probabilistici svolgano meglio il
compito di descrivere i fatti linguistici. La teoria _matematica_ dei
[linguaggi formali][] definisce il linguaggio come un insieme di frasi. Cioè,
ogni frase è grammaticalmente corretta oppure no; non c'è l'esigenza di 
probabilità in questo modello. Ma i linguaggi naturali non sono così. Una
teoria _scientifica_ dei linguaggi naturali deve tenere conto di tutte quelle
frasi che lascino TODO

1. The earth quaked.
2. ? It quaked her bowels

Eppure la (2) [appare][] [davvero][] come frase in inglese. Questo pone un
problema per le teorie fondate sulle categorie. Quando osserviamo la (2)
dobbiamo o scartarla come errore al di fuori del nostro modello (senza
argomentazioni teoriche per poterlo sostenere), o cambiare la teoria in modo
da ammetterla, il che spesso comporta accettare una serie di frasi che
avremmo preferito classificare come grammaticalmente scorrette. Come [disse][]
Edward Sapir nel 1921, "Tutte le grammatiche hanno perdite". Invece un modello
probabilistico non soffre di questo problema; possiamo dire che _quake_ ha una
probabilità alta di essere usato intransitivamente, e una bassa probabilità di
essere usato transitivamente (e, se ci interessa, possiamo descrivere questi
usi ulteriormente tramite sottocategorie).

Steve Abney [osserva][] che i modelli probabilistici sono più adatti a
modellare l'evoluzione del linguaggio. Cita l'esempio di un inglese del
quindicesimo secolo che si reca nel pub ogni giorno e ordina "ale!" [NdT: 
birra]. Seguendo un modello per categorie ci dovremmo ragionevolmente 
aspettare che un giorno gli vengano servite "eel" [NdT: anguille], perché il
[grande spostamento vocalico][] ha cambiato un parametro nella propria mente
il giorno prima che lo abbia cambiato in quella dell'oste. In un modello
probabilistico esisteranno più parametri, magari con valori continui, ed è
facile capire come uno spostamento possa accadere gradualmente in due secoli.

TODO

Consideriamo ora quello che penso essere il maggior punto di disaccordo fra
Chomsky e i modelli statistici: la tensione fra "descrizione accurata" e
"intuizione". Questa è una vecchia distinzione. Charles Darwin (biologo,
1809-1882) è maggiormente noto per TODO

D'altro canto Ernest Rutherford (fisico, 1871-1937) disprezzava la semplice
descrizione, dicendo "La scienza è fisica oppure collezione di francobolli".
Chomsky è con lui: "Puoi anche collezionare farfalle e fare tante
osservazioni. Se ti piacciono le farfalle va tutto bene, ma questo non deve
essere confuso con la ricerca, il cui scopo è la scoperta di principi
unificanti".

TODO

## Le due culture ##

Dopo aver dato spazio a tutti questi importanti scienziati, penso che il
contributo più rilevante a questa discussione sia stato dato nell'articolo
del 2001 di Leo Breiman (statistico, 1928-2005) dal titolo [Statistical Modeling: The Two Cultures][]. In questo articolo Breiman, con allusione a
C.P. Snow, descrive due culture:

Per prima la **cultura della modellazione con dati**, (a cui, stima Breiman,
appartiene il 98% degli statistici) la quale sostiene che la natura possa
essere descritta come una scatola nera con dentro un modello relativamente
semplice, che associ valori di input a valori di output (con magari un poco
di rumore aggiunto). Lo statistico ha il compito di scegliere accuratamente il
modello che rifletta la forma della realtà, e poi usare i dati statistici per
stimare i parametri del modello.

Seconda, la **cultura della modellazione algoritmica** (a cui appartengono
il 2% degli statistici e molti ricercatori in biologia, intelligenza
artificiale e altri campi che riguardino fenomeni complessi), la quale
sostiene che la scatola nera della natura non possa essere necessariamente
spiegata con modelli semplici. Metodi algoritmici complessi (come le Support
Vector Machines, Boosted Decision Trees, Deep Belief Networks) vengono usati
per stimare la funzione che mappi le variabili di input a quelle di output,
senza pensare che la _forma_ della funzione risultante rifletta la natura
sottostante.

Sembra che Chomsky sollevi critiche principalmente contro la cultura della
modellazione algoritmica. Non è solo perché i modelli sono statistici (o
probabilistici), è anche perché il loro prodotto modella accuratamente la
realtà ma non è interpretabile facilmente dagli uomini, e non vuole
corrispondere al processo verificatosi in natura. In altre parole, la
modellazione algoritmica descrive _cosa_ accade, ma non spiega _perché_.

TODO

Nel gennaio 2011 il personaggio televisivo Bill O'Reilly è intervenuto in più
di una guerra culturale con la propria frase ["La marea sale, la mare scende. Mai un malinteso. Nessuno è in grado di spiegarlo"][], a suo dire un argomento 
per l'esistenza di Dio. O'Reilly è stato ridicolizzato dai propri detrattori
per non sapere che le maree sono facilmente spiegate da un sistema di
equazioni alle derivate parziali che descrivono la mutua interazione fra sole,
terra e luna (un fatto [scoperto][] da Laplace nel 1776 e da allora
notevolmente raffinato; quando Napoleone gli chiese perché non avesse usato
un creatore nei suoi calcoli Laplace rispose "Non ebbi bisogno di tale 
ipotesi"). (O'Reilly sembra anche non conoscere Deimos e Phobos (due delle
mie lune preferite nel sistema solare, insieme con Europa, Io e Titano), o
che Marte e Venere orbitino intorno al sole, o neanche che Venere non
possiede lune perché, essendo talmente vicino al sole, c'è poco spazio per
delle orbite lunari stabili). Ma O'Reilly sa che non importa che cosa i
propri detrattori pensino della propria ignoranza astronomica, poiché sa che
i propri sostenitori invece pensano che sia arrivato esattamente al punto
chiave: _perché_? A lui non interessa _come_ funzionino le maree, ditegli
_perché_ funzionano. _Perché_ la luna è alla distanza giusta per determinare
maree non catastrofiche, e perché esercita un effetto stabilizzante sull'asse
di rotazione terrestre, proteggendone la vita? _Perché_ la gravita funziona
nel modo in cui funziona? _Perché_ esiste qualcosa invece di non esistere
niente? O'Reilly ha ragione nel sostenere che queste domande possono trovare
risposta solo nella creazione di miti, nella religione o nella filosofia, ma
non nella scienza.

La visione filosofica di Chomsky si basa sull'idea che dovremmo concentrarci
sui profondi _perché_ e che le semplici spiegazioni della realtà non contano.
In questo Chomsky è totalmente d'accordo con O'Reilly. (Mi rendo conto che la
precedente frase possieda una probabilità estremamente bassa in un modello
probabilistico addestrato su frasi provenienti da giornali o TV). Chomsky
crede che una teoria del linguaggio debba essere semplice e comprensibile,
come un modello di regressione lineare in cui la relazione da individuare è
una linea retta, e tutto quello che dobbiamo fare è stimare il coefficiente
angolare e l'intercetta.

Per esempio consideriamo la nozione di [linguaggio pro-drop][] esposta in
[Lectures on Government and Binding][] (1981) di Chomsky. In inglese diciamo
"I'm hungry", esprimendo il pronome "I". In spagnolo invece lo stesso pensiero
si traduce "Tengo hambre" (letteralmente "ho fame"), facendo cadere il pronome
"Yo". La teoria di Chomsky è che esista un "parametro pro-drop" che sia "vero"
per lo spagnolo e "falso" per l'inglese, e che, una volta scoperto un piccolo
insieme di parametri che descrivano tutti i linguaggi, e i valori per ciascuno
di essi, avremo ottenuto la vera comprensione.

Il problema è che la realtà è più complicata della sua teoria. Ecco dei
pronomi mancanti in inglese:

  + "Not gonna do it. Wouldn't be prudent." (Dana Carvey, [imitando George H. W. Bush][])
  + "Thinks he can outsmart us, does he?" (Evelyn Waugh, [The Loved One][])
  + "Like to fight, does he?" (S.M. Stirling, [The Sunrise Lands][])
  + "Thinks he's all that." (Kate Brian, [Lucky T][])
  + "Go for a walk?" (infiniti proprietari di cani)
  + "Gotcha!" "Found it!" "Looks good to me!" (espressioni comuni)

I linguisti possono discutere sull'interpretazione di questi fatti per ore, ma
la diversità dei linguaggi sembrano molto più complesse di un singolo valore
booleano di un parametro pro-drop. Non dovremmo accettare una struttura
teorica che prioritizzi un modello semplice a uno che rifletta accuratamente
la realtà.

Sin dall'inizio, Chomsky si è concentrato sugli aspetti _generativi_ del
linguaggio. Da questo punto di vista è ragionevole TODO

Infine, un'ulteriore ragione per cui a Chomsky non piacciono i modelli
statistici è che tendono a rendere la linguistica una scienza empirica (una
scienza su come le persone usano davvero il linguaggio) piuttosto che una
scienza matematica (uno studio delle proprietà matematiche dei modelli del
linguaggio formale). Chomsky preferisce la seconda, come evidente da questa
frase in [Aspects of the Theory of Syntax][] (1965):

> La teoria linguistica è mentalista, poiché si preoccupa di scoprire una
> realtà mentale sottesa al comportamento osservato. L'osservazione dell'uso
> del linguaggio ... può fornire dati ... ma sicuramente non può essere il
> contenuto della linguistica, affinché possa essere una disciplina
> rispettabile.

Non riesco a immaginare Laplace che affermi che l'osservazione dei pianeti non
sia l'argomento della meccanica orbitale, o Maxwell che dica che le
osservazioni delle cariche non lo siano per l'elettromagnetismo. È vero che la
fisica si occupa di casi ideali che sono astrazioni del complicato mondo
reale. Per esempio, certi problemi di meccanica ignorano l'attrito. Ma questo
non significa che l'attrito non possa essere considerato parte della fisica.

E quindi come può Chomsky affermare che l'osservazione del linguaggio non
possa essere l'argomento della linguistica? Sembra che venga dal proprio punto
di vista di [platonista][] e [razionalista][] e forse anche un po'
[mistico][]. Chomsky pensa, come nel mito della caverna di Platone, che ci
dovremmo concentrare sulle forme astratte e ideali sottese al linguaggio, non
sulle manifestazioni esteriori di esso percepibili nel mondo reale. Ecco
perché non è interessato all'uso del linguaggio. Ma Chomsky, così come
Platone, deve saper spiegare l'origine di queste forme ideali. Chomksy (1991)
dimostra di accontentarsi della spiegazione mistica, sebbene non parli di
"anima" ma di "dote biologica".

> La risposta di Platone era che la conoscenza è "ricordata" da un'esistenza
> precedente. La risposta richiede un meccanismo: magari un'anima immortale
> ... riformulando la risposta di Platone in termini più congeniali a quelli
> moderni, diremo che le proprietà basilari dei sistemi cognitivi sono innate
> nella mente, parte della dote biologica umana.

Era ragionevole che Platone pensasse che l'ideale di, diciamo, un cavallo 
fosse più importante di ciascun cavallo che possa essere percepito. Nel 400 
a.C. le specie erano ritenuto eterne e fisse. Oggi sappiamo che non è vero; i 
cavalli di un altro muro di caverna — quelli di Lascaux — sono oggi estinti, e 
che i cavalli odierni continuano a evolvere nel tempo. Quindi non esiste 
qualcosa come una singola, eterna, forma ideale di "cavallo". 

Oggi sappiamo che anche i linguaggi sono così: sono processi complessi,
casuali, contingenti e biologici, soggetti ai capricci dell'evoluzione e della
cultura. Un linguaggio non è una forma eterna e ideale rappresentabile da un
piccolo insieme di parametri, anzi è il risultato di processi complicati.
Poiché è contingente, sembra che possa essere analizzato soltanto con modelli
probabilistici. Poiché inoltre le persone devono continuamente capire il
linguaggio ambiguo e incerto degli altri, sembra che debba essere usato
del ragionamento probabilistico. Chomksy per qualche ragione vuole evitarlo,
e quindi deve dichiarare che certi fatti dell'uso del linguaggio sono non
grammaticali, e che la vera linguistica esiste soltanto nell'astrazione
matematica, dove può imporre il formalismo che preferisce. Poi per trasferire
il linguaggio da questo mondo eterno, astratto e matematico alle teste delle
persone deve inventarsi un meccanismo mistico pensato apposta per questo
mondo eterno. Questo può essere molto interessante da un punto di vista
matematico, ma non chiarisce che cosa sia un linguaggio e come funzioni.

## Bibliografia annotata ##

1. Abney, Steve (1996) [Statistical Methods and Linguistics][], in Klavans e Resnick (ed.) _The Balancing Act: Combining Symbolic and Statistical Approaches to Language_, MIT Press.
   > Un'introduzione eccellente e completa all'approccio statistico per la
   > trattazione del linguaggio, che inoltre comprende alcuni argomenti non
   > trattati spesso come l'evoluzione del linguaggio e le differenze
   > individuali.
   
2. Breiman, Leo (2001) [Statistical Modeling: The Two Cultures][], _Statistical Science_, Vol. 16, No. 3, 199-231.
   > Breiman descrive magnificamente i due approcci, spiegando i benefici del
   > proprio e difendendo i propri argomenti con le testimonianze di eminenti
   > statistici: Cox, Efron, Hoadley e Parzen.
   
3. Chomsky, Noam (1956) [Three Models for the Description of Language][], _IRE Transactions on Information theory_ (2), pp. 113-124.
   > Vengono comparate le grammatiche a stati finiti, a struttura di frase e
   > trasformazionali. Viene introdotta la frase "colorless green ideas sleep 
   > furiosly".
   
4. Chomsky, Noam (1967) [Syntactic Structures][], Mouton.
   > Un'esposizione in formato libro della teoria di Chomsky che fu la
   > dottrina principale in linguistica per un decennio. Viene sostenuto che
   > i modelli probabilistici non forniscono intuizione per la sintassi.
   
5. Chomsky, Noam (1969) [Some Empirical Assumptions in Modern Philosophy of Language][], in _Philosophy, Science and Method: Essays in Honor or Ernest Nagel_, St. Martin's Press.
   > Viene sostenuto che la nozione di "probabilità di una frase" sia
   > totalmente inutile.
   
6. Chomsky, Noam (1981) [Lectures on government and binding][], de Gruyer.
   > Una revisione della teoria di Chomsky; viene introdotta la Grammatica
   > Universale. La citiamo per l'inclusione di parametri come "pro-drop".
   
7. Chomsky, Noam (1991) [Linguistics and adjacent fields: a personal view][], in Kasher (ed.), _A Chomskyan Turn_, Oxford.
   > Ho trovato le citazioni su Platone in questo articolo pubblicato dal
   > Partito Comunista della Gran Bretagna e scritto da una persona 
   > apparentemente a digiuno di concetti di linguistica ma con una precisa 
   > agenda politica.
   
8. Gold, E. M. (1967) [Language Identification in the Limit][], _Information and Control_, Vol. 10, No. 5, pp. 447-474.
   > Gold dimostrò un risultato nella teoria dei linguaggi formali che
   > possiamo così enunciare (con qualche licenza): supponete di avere un
   > gioco fra due giocatori, chi indovina e chi sceglie. Il secondo dice al
   > primo "Ecco un numero infinito di linguaggi. Ne sceglierò uno e ti
   > leggerò frasi da questo linguaggio. A un certo tuo compleanno ti darò un
   > test del tipo Vero/Falso composto da 100 frasi che non hai ancora
   > sentito e tu dovrai dire se quelle frasi appartengono al linguaggio
   > oppure no." Ci sono certe condizioni su come sia fatto questo insieme 
   > infinito e come possano essere scelte le frasi (può volontariamente 
   > sviare, ma non può ad esempio ripetere la stessa frase per sempre). Il 
   > risultato di Gold è che se l'insieme infinito è composto da linguaggi 
   > liberi da contesto allora non esiste una strategia per cui chi indovina 
   > sia in grado di dare la risposta giusta a tutte le 100 domande, non 
   > importa quanto in là sia il giorno del test. Chomsky e altri hanno 
   > interpretato questo risultato come affermante che è impossibile che un
   > bambino impari il linguaggio umano senza possedere un innato "organo del 
   > linguaggio". Ma, come [Johnson (2004)][] e altri dimostrano, questa è una 
   > conclusione non valida; il compito di azzeccare tutte le domande del test 
   > (definito da Gold "Identificazione del linguaggio") non ha in realtà 
   > niente a che vedere con il compito di acquisizione del linguaggio svolto 
   > dai bambini, quindi il teorema di Gold non si applica.
     
9. Horning, J. J. (1969) [A study of grammatical inference][], Tesi di Dottorato, Stanford Univ.
   > Se Gold trovò un risultato negativo — cioè che i linguaggi liberi da
   > contesto non sono identificabili tramite esempi — Horning invece trovò
   > un risultato positivo — cioè che i linguaggi probabilistici liberi da
   > contesto invece lo sono (a meno di un errore arbitrariamente piccolo).
   > Nessuno dubita che gli uomini posseggano una capacità unica e innata di
   > comprendere il linguaggio (sebbene sia ignoto in quale misura queste
   > capacità siano specifiche per il linguaggio o siano abilità cognitive
   > generiche di produzione di astrazioni e ordinamento). Horning tuttavia 
   > dimostrò nel 1969 che il teorema di Gold non possa essere usato come 
   > argomento convincente per sostenere l'esistenza di un organo del 
   > linguaggio innato, il quale specifichi l'intero linguaggio tranne qualche 
   > parametro.
   
10. Johnson, Kent (2004) [Gold's Theorem and cognitive science][], _Philosophy of Science_, Vol. 71, pp. 571-592.
    > Il miglior articolo che abbia letto sul vero contenuto del teorema di
    > Gold e su quello che è stato detto in proposito (correttamente e non).
    > Si conclude che il teorema di Gold riguarda il linguaggi formali, ma non
    > l'acquisizione del linguaggio.

11. Lappin, Shalom and Shieber, Stuart M. (2007) [Machine learning theory and practice as a source of insight into universal grammar][], _Journal of Linguistics_, Vol. 43, No. 2, pp. 393-427.
    > Un articolo eccellente che discute la povertà dello stimolo, il fatto 
    > che tutti i modelli non siano imparziali, la differenza fra 
    > apprendimento supervisionato e non, le teorie moderne di apprendimento 
    > (PAC e VC). Propone alternative al modello della Grammatica Universale 
    > consistente di un insieme fissato di parametri binari.

12. Manning, Christopher (2002) [Probabilistic Syntax][], in Bod, Hay, e Jannedy (ed.), _Probabilistic Linguistics_, MIT Press.
    > Un'introduzione affascinante alla sintassi probabilistica, e a come sia
    > un modello migliore per fatti linguistici rispetto alla sintassi per
    > categorie. Contiene inoltre "le gioie e i pericoli della linguistica su
    > corpus".

13. Norvig, Peter (2007) [How to Write a Spelling Corrector][], pagina web.
    > Mostra del codice funzionante per implementare un algoritmo di 
    > correzione ortografica probabilistico e statistico.

14. Norvig, Peter (2009) [Natural Language Corpus Data][], in Seagran and Hammerbacher (ed.), _Beautiful Data_, O'Reilly.
    > Una versione espansa del precedente articolo; mostra come implementare
    > tre compiti: divisione del testo, decodifica crittografica e correzione
    > ortografica (in modo leggermente più completo del precedente articolo).

15. Pereira, Fernando (2002) [Formal grammar and information theory: together again?][], in Nevin and Johnson (ed.), _The Legacy of Zellig Harris_, Benjamins.
    > Quando ho cominciato a scrivere il post che state leggendo in questo
    > momento mi sono concentrato su eventi che accadevano a Cambridge,
    > Massachusetts, a 4800 chilometri da casa. Dopo aver fatto un po' di
    > ricerca mi ha sopreso scoprire che gli autori di due dei tre migliori         articoli su questo argomento sedevano a 10 metri dalla mia scrivania:
    > Fernando Pereira e Chris Manning. (Il terzo, Steve Abney, sta a 3700
    > chilometri di distanza.) Ma forse non mi sarei dovuto stupire. Ricordo
    > una volta in cui tenni una conferenza all'Associazione dei Linguisti
    > Computazionali sui modelli del linguaggio usati a Google basati su 
    > corpus, e Fernando, allora professore all'università della Pennsylvania,
    > commentò "Mi sento come se io fossi un fisico delle particelle e tu
    > avessi l'unico acceleratore esistente". Qualche anno dopo arrivò a
    > Google. Fernando è anche famoso per la frase "Più vecchio divento, più
    > giù vado nella Gerarchia di Chomsky". Il suo articolo parla
    > sostanzialmente delle stesse cose del presente post, ma si spinge più in
    > profondità nello spiegare i vari modelli probabilistici disponibili e
    > in che modo siano utili.

16. Platone (c. 380AC) [La Repubblica][]
    > Qui citato per il mito della caverna.

17. Shannon, C.E. (1948) [A Mathematical Theory of Communication][], _The Bell System Technical Journal_, Vol. 27, pp. 379-423, 623-656.
    > Un articolo enormemente influente che dette inizio al campo della Teoria
    > dell'informazione, introdusse il termine "bit" e il modello di canale con rumore, dimostrò l'approssimazione dell'inglese per n-grammi,
    > descrisse modelli Markoviani del linguaggio, definì l'entropia rispetto
    > a questi modelli, e permise la crescita dell'industria delle
    > telecomunicazioni.

["On Chomsky and the Two Cultures of Statistical Learning"]: http://norvig.com/chomsky.html

[sostiene]: http://www.technologyreview.com/computing/37525/?a=f
["Brain, Minds, and Machines"]: http://mit150.mit.edu/symposia/brains-minds-machines
[trascrizione]: http://languagelog.ldc.upenn.edu/myl/PinkerChomskyMIT.html
[propose modelli probabilistici della comunicazione]: http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf
[sintassi probabilistica]: http://nlp.stanford.edu/~manning/papers/probsyntax.pdf
[legge dei gas perfetti]: http://it.wikipedia.org/wiki/Equazione_di_stato_dei_gas_perfetti
[corpus di mille miliardi di parole]: http://norvig.com/ngrams/
[dieci volte più accurato]: http://norvig.com/spell-correct.html

[scrisse]: http://www.vinartus.net/spa/95c.pdf

[definizione da dizionario]: http://www.google.com/webhp?sourceid=chrome-instant&ie=UTF-8&ion=1&nord=1#sclient=psy&hl=en&tbo=1&nord=1&tbs=dfn:1&source=hp&q=science&aq=f&aqi=p-p2g-e2g-c2g2g-c1g1&aql=&oq=&pbx=1&tbo=1&bav=on.2,or.r_gc.r_pw.&fp=18c4c2d0f0f5fea9&biw=1186&bih=634
[Science]: http://www.sciencemag.org/
[Chlorinated Indium Tin Oxide Electrodes with High Work Function for Organic Device Compatibility]: http://www.sciencemag.org/content/332/6032/944.abstract
[1]: http://www.sciencemag.org/content/332/6032.toc
[2]: http://www.cell.com/issue?pii=S0092-8674(11)X0010-7
[Premi Nobel 2010]: http://nobelprize.org/nobel_prizes/lists/year/

[abbia scritto]: http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false
[Considerate:]: http://books.google.com/books?id=BANsZLg1uy4C&lpg=PP1&dq=unfortunate%20events%20reptile&pg=PA135#v=onepage&q=ever%20ever%20ever&f=false
[Current Opinion: Volume 52]: http://books.google.com/books?id=fTciAQAAIAAJ&pg=PA96&dq=%22ideas+sleep%22&hl=en&ei=bZrcTePRBeHmiAKp2fHoDw&sa=X&oi=book_result&ct=result&resnum=3&sqi=2&ved=0CDoQ6AEwAg#v=onepage&q=%22ideas%20sleep%22&f=false

[Statistical Methods and Linguistics]: http://www.vinartus.net/spa/95c.pdf
[Statistical Modeling: The Two Cultures]: http://projecteuclid.org/DPubS?service=UI&version=1.0&verb=Display&handle=euclid.ss/1009213726
[Three Models for the Description of Language]: http://www.chomsky.info/articles/195609--.pdf
[Syntactic Structures]: http://books.google.com/books?id=SNeHkMXHcd8C&printsec=frontcover&dq=syntactic+structures+chomsky&hl=en&src=bmrr&ei=1WvcTa3UDeLQiAK6-4zpDw&sa=X&oi=book_result&ct=book-thumbnail&resnum=1&ved=0CD4Q6wEwAA#v=onepage&q=probabilistic&f=false
[Some Empirical Assumptions in Modern Philosophy of Language]: http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false
[Lectures on government and binding]: http://books.google.com/books?id=l08tpkOOdNQC&printsec=frontcover&source=gbs_ge_summary_r&cad=0#v=onepage&q=pro-drop&f=false
[Linguistics and adjacent fields: a personal view]: http://www.cpgb.org.uk/article.php?article_id=1004261#23
[Language Identification in the Limit]: http://en.wikipedia.org/wiki/Language_identification_in_the_limit
[Johnson (2004)]: http://psyling.psy.cmu.edu/papers/years/2004/logical/gold-johnson.pdf
[A study of grammatical inference]: http://portal.acm.org/citation.cfm?id=905718&coll=DL&dl=GUIDE&CFID=23829333&CFTOKEN=19917759
[Gold's Theorem and cognitive science]: http://psyling.psy.cmu.edu/papers/years/2004/logical/gold-johnson.pdf
[Machine learning theory and practice as a source of insight into universal grammar]: http://www.dcs.kcl.ac.uk/staff/lappin/papers/lappin-shieber_learning07.pdf
[Probabilistic Syntax]: http://nlp.stanford.edu/~manning/papers/probsyntax.pdf
[How to Write a Spelling Corrector]: http://norvig.com/spell-correct.html
[Natural Language Corpus Data]: http://norvig.com/ngrams/
[Formal grammar and information theory: together again?]: http://www.cis.upenn.edu/~pereira/papers/rsoc.pdf
[La Repubblica]: http://en.wikipedia.org/wiki/Allegory_of_the_Cave
[A Mathematical Theory of Communication]: http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf