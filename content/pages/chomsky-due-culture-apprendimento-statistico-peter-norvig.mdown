Date: 03 Jul 2012
Summary:

# Su Chomsky e le due culture dell'apprendimento statistico #

_Questo post è una traduzione di ["On Chomsky and the Two Cultures of Statistical Learning"][] di Peter Norvig._

Technology Review [sostiene][] che al simposio dal titolo 
["Brain, Minds, and Machines"][] tenuto in occasione dei 150 anni del MIT il
professor Noam Chomsky

> ha deriso i ricercatori in apprendimento automatico che usano metodi
> puramente statistici per produrre comportamenti che imitino qualcosa di
> esistente nel mondo, ma che non provano a comprendere il significato di tali
> comportamenti.

Ora è disponibile la [trascrizione][], perciò citiamo Chomsky stesso:

> È vero che è stato fatto un sacco di lavoro nel provare ad applicare modelli
> statistici a svariati problemi linguistici. Penso che ci sia stato qualche
> successo, ma molti fallimenti. Esiste una nozione di successo ... che penso
> si sia vista per la prima volta nella storia della scienza. Chiamano 
> "successo" l'aver approssimato dati non ancora analizzati.

Questo post discute le affermazioni di Chomsky, specula su quello che avrebbe
potuto intendere e prova a determinarne fondatezza e importanza.

Le osservazioni di Chomsky erano in risposta alla domanda di Steven Pinker sui
successi ottenuti dai modelli probabilistici addestrati con metodi statistici.

1. Cosa intendeva Chomsky? Ha ragione?
2. Cosa è un modello statistico?
3. Quanto successo hanno avuto i modelli statistici del linguaggio?
4. Esiste una nozione comparabile di successo nella storia della scienza?
5. Cosa non piace a Chomsky dei modelli statistici?

## Cosa intendeva Chomsky? Ha ragione? ##

Penso che gli argomenti di Chomsky siano i seguenti:

1. I modelli statistici del linguaggio hanno avuto un successo in senso
ingegneristico, ma questo è irrilevante per la scienza.

2. Modellare accuratamente fatti linguistici è come collezionare farfalle;
ciò che conta nella scienza (e in particolare nella linguistica) sono i
principi sottostanti.

3. I modelli statistici sono incomprensibili; non sono di aiuto
all'intuizione.

4. I modelli statistici possono simulare accuratamente certi fenomeni, ma la
simulazione è fatta in modo totalmente sbagliato; le persone non decidono
quale dovrebbe essere la terza parola di una frase consultando una tabella di
probabilità TODO

5. I modelli statistici si sono dimostrati incapaci di apprendere il
linguaggio; dunque il linguaggio deve essere innato, perciò perché questi
modellatori statistici stanno sprecando il proprio tempo in uno sforzo
sbagliato?

Ha ragione? Questo è un dibattito di lunga data. Di seguito le mie risposte:

1. Sono d'accordo che il successo ingegneristico non sia l'obiettivo o la
misura di buona scienza. Osservo però che la scienza e l'ingegneria si 
sviluppano insieme, e che il successo ingegnerestico indica che qualcosa
sta andando per il verso giusto, perciò è un indizio (ma non una prova) di
un modello scientifico di successo.

2. La scienza è una combinazione di raccolta di dati e formulazione di teorie;
nessuna delle due attività può procedere da sola. Penso che Chomsky abbia 
torto nel far pendere la bilancia a favore della teoria a discapito della
raccolta di dati; nella storia della scienza, TODO

3. Sono d'accordo che sia difficile comprendere un modello contenente miliardi
di parametri. Sicuramente un uomo non può capire un tale modello andando a
indagare individualmente i valori di ogni parametro. Ma si può guadagnare in
intuizione esaminando le _proprietà_ del modello — dove ha successo e dove fallisce, quanto bene impara in funzione dei dati, eccetera.

4. Sono d'accordo che un modello Markoviano di probabilità delle singole
parole non può modellare l'intero linguaggio. È ugualmente vero che un conciso
modello ad albero privo di probabilità non può modellare l'intero linguaggio.
Serve un modello probabilistico che copra parole, alberi, semantica, contesti,
discorsi, eccetera. Chomsky rigetta tutti i modelli probabilistici per via
delle mancanze di particolari modelli di 50 anni fa. Capisco come Chomsky,
partendo dalla studio della generazione del linguaggio, arrivi alla 
conclusione che i modelli probabilistici non siano necessari. Ma la stragrande
maggioranza delle persone che studiano compiti di _interpretazione_, come il
riconoscimento della voce, TODO

5. Nel 1967 il teorema di Gold mostrò alcune limitazioni teoriche sulla
deduzione logica di linguaggi formali. Ma questo risultato non ha niente a che 
vedere con il compito affrontato da chi impara un linguaggio naturale. In ogni
caso, nel 1969 sapevamo già, grazie a Horning, che l'inferenza probabilistica 
di grammatiche libere da contesto non soffre di tale limitazione. Sono 
d'accordo con Chomsky che sia innegabile che gli uomini possiedano una 
capacità naturale di imparare il linguaggio naturale, ma non sappiamo 
abbastanza di questa capacità per escluderne rappresentazioni probabilistiche 
o apprendimento statistico. Penso che sia molto più probabile che 
l'apprendimento del linguaggio umano comporti qualcosa simile all'inferenza 
probabilistica o statistica, ma non lo sappiamo ancora.

Consentitemi ora di giustificare le mie risposte con uno sguardo più attento
alle restanti domande.

## Cosa è un modello statistico? ##

Un **modello statistico** è un modello matematico modificato o addestrato 
dall'aggiunta di dati. I modelli statistici sono spesso, ma non sempre,
probabilistici. Dove è importante distinguere saremo attenti a non dire
soltanto "statistico", ma useremo i seguenti lemmi:

  + Un **modello matematico** specifica una relazione fra variabili, sia in 
  una forma funzionale TODO
  + Un **modello probabilistico** TODO
  + Un **modello addestrato** TODO

Ad esempio, un decennio prima di Chomsky, Claude Shannon [propose modelli probabilistici della comunicazione][] basati su catene di Markov di parole. Se
avete un vocabolario di $100000$ parole e un modello di Markov del secondo
ordine (cioè in cui la probabilità di una parola dipende dalle precedenti
due), avrete bisogno di specificare $10^ {15}$ probabilità per determinare il
modello. L'unica maniera fattibile per imparare questi $10^ {15}$ parametri
consiste nel raccogliere statistiche dai dati e usare un qualche metodo di
approssimazione per i molti casi in cui non esistono dati. Perciò molti, ma
non tutti, modelli probabilistici sono addestrati. Inoltre molti, ma non
tutti, modelli addestrati sono probabilistici.

Come ulteriore esempio, consideriamo il modello Newtoniano dell'attrazione
gravitazionale, il quale afferma che la forza fra due oggetti di masse $m_1$ e
$m_2$ a distanza $r$ sia data da 

$$F = G\frac{m_1 m_2}{r^ 2}$$

dove $G$ è la costante di gravitazione universale. Questo è un modello
addestrato perché tale costante è stata determinata statisticamente per mezzo
di batterie di esperimenti che soffrivano di errore sperimentale stocastico. È
anche un modello deterministico (non probabilistico) perché stabilisce
un'esatta relazione funzionale. Credo che Chomsky non abbia obiezioni verso
questo tipo di modello statistico. Piuttosto, sembra rivolgere le proprie
critiche a modelli statistici come quello di Shannon che hanno miliardi e
miliardi di parametri, non soltanto uno o due.

(Questo esempio ci permette di introdurre un'ulteriore distinzione: il modello
gravitazionale è **continuo** e **quantitativo**, mentre invece la tradizione
linguistica ha favorito modelli che siano **discreti**, **categorici** e
**qualitativi**: una parola è o meno un verbo, non ci chiediamo quanto lo sia.
Per maggiori dettagli su questa distinzione, consultate l'articolo di Chris 
Manning sulla [Sintassi Probabilistica][].)

Un modello probabilistico e statistico di interesse è la [legge dei gas perfetti], che descrive la pressione $P$ di un gas in termini del numero di
molecole $N$, della temperatura $T$ e della costante di Boltzmann $k$:

$$P = \frac{Nk\,T}{V}\text{.}$$

Questa equazione può essere ricavata da principi primi con i mezzi della
meccanica statistica. È un modello stocastico e non esatto; il _vero_ modello
dovrebbe describere i moti di ogni singola particella di gas. Questo modello
ignora tale complessità e _riassume_ TODO

Ora invece consideriamo il modello non statistico dell'ortografia inglese
espresso dalla regola _"I before E except after C."_ Confrontiamolo con il 
seguente modello probabilistico, addestrato e statistico:

$$
\begin{align}
  P(\text{IE})& =0.0177& P(\text{CIE})& =0.0014& P(\text{\*IE})& =0.163&\\\
  P(\text{EI})& =0.0046& P(\text{CEI})& =0.0005& P(\text{\*EI})& =0.0041&
\end{align}
$$

Questo modello deriva dall'analisi statistica di un [corpus di mille miliardi di parole][] inglesi. La notazione $P(\text{IE})$ indica la probabilità che 
una parola di questo contenga le lettere consecutive $\text{IE}$. Allo stesso
modo $P(\text{CIE})$ denota la probabilità che una parola contenga le lettere
$\text{CIE}$, mentre $P(\text{\*IE})$ indica la probabilità di qualunque
lettera diversa da $\text{C}$ seguita da $\text{IE}$. I dati confermano che
$\text{IE}$ è effettivamente più comune di $\text{EI}$, e che il dominio di
$\text{IE}$ si attenua quando preceduto da $\text{C}$, ma, contrariamente alla
regola, $\text{CIE}$ è comunque più comune di $\text{CEI}$. Esempi di parole
contenenti $\text{CIE}$ sono "science", "society", "ancient", "species". Lo
svantaggio della regola _"I before E except after C"_ è che TODO

$$
\begin{align}
  \text{TODO}
\end{align}
$$

Un modello statistico più complesso (diciamo, uno che dia le probabilità di
tutte le sequenze di 4 lettere e/o di tutte le parole conosciute) potrebbe
essere TODO

Come ultimo esempio (non di modelli statistici, ma per aiutare l'intuzione),
consideriamo la Teoria delle Strette di Mano fra Giudici della Corte Suprema:
quando si riuniscono, tutti i giudici stringono la mano con ogni altro 
giudice. Il numero di partecipanti, $n$, deve essere un intero compreso fra 
0 e 9; qual è il numero totale di strette di mano $h$ per un dato $n$?
Ecco tre possibili spiegazioni:

1. Ognuno degli $n$ giudici stringe la mano agli altri $n-1$ giudici, ma così
stiamo contando le strette di mano fra Alito/Breyer e Breyer/Alito come due
strette di mano distinte, perciò dobbiamo dividere il totale a metà, e quindi
otteniamo $h = \frac{n(n-1)}{2}$.

2. Per evitare di contare due volte ogni stretta ordiniamo i giudici per età e
contiamo TODO

3. Consultiamo questa tabella: TODO

Alcune persone preferiranno la prima spiegazione, alcune la seconda, e chi è
lento a fare addizioni e moltiplicazioni potrebbe preferire la terza. Perché?
Tutte e tre le spiegazioni descrivono _esattamente la stessa teoria_ — la
stessa funzione da $n$ a $h$, definita sullo stesso dominio di possibili
valori di $n$. Quindi potremmo preferire 

## Quanto successo hanno avuto i modelli statistici del linguaggio? ##

Chomsky ha detto chi i modelli statistici hanno avuto un successo limitato in
certe aree applicative. Andiamo a vedere come i computer approcciano il
linguaggio, e definiamo "successo" come "fare previsioni accurate sul mondo"
Prima ancora le maggiori aree applicative:

  + **Motori di ricerca** TODO
  + TODO
  + **Traduzione automatica** TODO
  + TODO

Ora invece volgiamo l'attenzione alle parti che sono d'interesse per il
linguista computazionale e non l'utente tipico:

  + TODO
  + TODO
  + TODO
  + TODO

Chiaramente non è accurato dire che i modelli statistici (e probabilistici)
hanno ottenuto un successo _limitato_; piuttosto hanno raggiunto una posizione
_dominante_ (ma non esclusiva).

TODO

TODO

TODO

TODO

## Esiste una nozione comparabile di successo nella storia della scienza? ##

Quando Chomsky ha detto _"Questa è una nozione di successo scientifico che
è totalmente nuova. Non conosco niente di simile nella storia della scienza"_
intendeva che TODO

Una [definizione da dizionario][] della scienza è "Lo studio sistematico della
struttura e del comportamento del mondo fisico e naturale attraverso 
osservazioni e esperimenti", il che enfatizza TODO

> TODO

Sembra proprio che questo articolo si concentri sul "modellare accuratamente
la realtà" piuttosto che "aiutare l'intuizione". TODO

Poi ho guardato a tutti i titoli e abstract del [numero corrente][1] di 
_Science_:

  + TODO
  + TODO
  + TODO

e poi ho fatto lo stesso per il [numero corrente][2] di _Cell_:

  + TODO
  + TODO
  + TODO
  
e infine per i [Premi Nobel 2010][] nelle scienze:

  + Fisica: TODO
  + Chimica: TODO
  + Fisiologia o Medicina: TODO

La mia conclusione è che il 100% di questi premi e articoli riguardano più che
altro "modellare accuratamente il mondo" piuttosto che "aiutare l'intuizione",
sebbene tutti TODO

## Cosa non piace a Chomsky dei modelli statistici? ##

Ho detto che i modelli statistici vengono talvolta confusi con i modelli
probabilistici; consideriamo dunque in quale misura le obiezioni di Chomsky
siano piuttosto rivolte verso i modelli probabilistici. È ben noto che nel 
1969 [abbia scritto][]:

> Ma deve essere riconosciuto che la nozione di "probabilità di una frase" sia
> interamente inutile, sotto qualunque sua interpretazione.

Il suo argomento principale era che TODO

> Penso che siamo costretti a concludere che ... i metodi probabilistici TODO

TODO

1. TODO
2. TODO
3. TODO
4. TODO

TODO

TODO

TODO

> Nè (a) 'colorless green ideas sleep furiosly' nè (b) 'furiosly sleep ideas
> green colorless', o una qualunque delle loro parti, sono mai apparse nell'
> esperienza linguistica di chi parla inglese. Ma (a) è grammaticalmente
> corretta, (b) no.

TODO

  + TODO
  + TODO
  + "**Ideas sleep** in books." [Current Opinion: Volume 52][], (1912)

TODO

TODO

TODO

TODO

1. The earth quaked.
2. ? It quaked her bowels

TODO

TODO

TODO

TODO

TODO

TODO

## Le due culture ##

TODO

## Bibliografia annotata ##

1. Abney, Steve (1996) [Statistical Methods and Linguistics][], in Klavans e Resnick (ed.) _The Balancing Act: Combining Symbolic and Statistical Approaches to Language_, MIT Press.
   > TODO
   
2. TODO
   > TODO
   
3. TODO
   > TODO
   
4. Chomsky, Noam (1967) [Syntactic Structures][], Mouton.
   > 
   
5. Chomsky, Noam (1969) [Some Empirical Assumptions in Modern Philosophy of Language][], in _Philosophy, Science and Method: Essays in Honor or Ernest Nagel_, St. Martin's Press.
   > Viene sostenuto che la nozione di "probabilità di una frase" sia
   > totalmente inutile.
   
6. TODO
   > TODO
   
7. TODO
   > TODO
   
8. TODO
   > TODO
     
9. TODO
   > TODO
   
10. TODO
    > TODO

11. TODO
    > TODO

12. TODO
    > TODO

13. TODO
    > TODO

14. TODO
    > TODO

15. Pereira, Fernando (2002) [Formal grammar and information theory: together again?][], in Nevin and Johnson (ed.), _The Legacy of Zellig Harris_, Benjamins.
    > Quando ho cominciato a scrivere il post che state leggendo in questo
    > momento mi sono concentrato su eventi che accadevano a Cambridge,
    > Massachusetts, a 4800 chilometri da casa. Dopo aver fatto un po' di
    > ricerca mi ha sopreso scoprire che gli autori di due dei tre migliori         articoli su questo argomento sedevano a 10 metri dalla mia scrivania:
    > Fernando Pereira e Chris Manning. (Il terzo, Steve Abney, sta a 3700
    > chilometri di distanza.) Ma forse non mi sarei dovuto stupire. Ricordo
    > una volta in cui tenni una conferenza all'Associazione dei Linguisti
    > Computazionali sui modelli del linguaggio usati a Google basati su 
    > corpus, e Fernando, allora professore all'università della Pennsylvania,
    > commentò "Mi sento come se io fossi un fisico delle particelle e tu
    > avessi l'unico acceleratore esistente". Qualche anno dopo arrivò a
    > Google. Fernando è anche famoso per la frase "Più vecchio divento, più
    > giù vado nella Gerarchia di Chomsky". Il suo articolo parla
    > sostanzialmente delle stesse cose del presente post, ma si spinge più in
    > profondità nello spiegare i vari modelli probabilistici disponibili e
    > in che modo siano utili.

16. Platone (c. 380AC) [La Repubblica][]
    > Qui citato per il mito della caverna.

17. Shannon, C.E. (1948) [A Mathematical Theory of Communication][], _The Bell System Technical Journal_, Vol. 27, pp. 379-423, 623-656.
    > TODO

["On Chomsky and the Two Cultures of Statistical Learning"]: http://norvig.com/chomsky.html

[sostiene]: http://www.technologyreview.com/computing/37525/?a=f
["Brain, Minds, and Machines"]: http://mit150.mit.edu/symposia/brains-minds-machines
[trascrizione]: http://languagelog.ldc.upenn.edu/myl/PinkerChomskyMIT.html
[propose modelli probabilistici della comunicazione]: http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf
[Sintassi Probabilistica]: http://nlp.stanford.edu/~manning/papers/probsyntax.pdf
[legge dei gas perfetti]: http://it.wikipedia.org/wiki/Equazione_di_stato_dei_gas_perfetti
[corpus di mille miliardi di parole]: http://norvig.com/ngrams/

[abbia scritto]: http://books.google.com/books?id=iaXVXYDQN1oC&pg=PA296&dq=%22probability+of+a+sentence%22+%22entirely+useless%22&hl=en&ei=_s7eTc_5DrTciALcsvnlCg&sa=X&oi=book_result&ct=result&resnum=4&ved=0CDoQ6AEwAw#v=onepage&q=%22probability%20of%20a%20sentence%22%20%22entirely%20useless%22&f=false
[Current Opinion: Volume 52]: http://books.google.com/books?id=fTciAQAAIAAJ&pg=PA96&dq=%22ideas+sleep%22&hl=en&ei=bZrcTePRBeHmiAKp2fHoDw&sa=X&oi=book_result&ct=result&resnum=3&sqi=2&ved=0CDoQ6AEwAg#v=onepage&q=%22ideas%20sleep%22&f=false

[Statistical Methods and Linguistics]: http://www.vinartus.net/spa/95c.pdf

[La Repubblica]: http://en.wikipedia.org/wiki/Allegory_of_the_Cave